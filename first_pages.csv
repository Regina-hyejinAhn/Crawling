0
"THEPOTENTIAL AND VALUE OF AI C HATBOT IN PERSONALIZED
COGNITIVE TRAINING
A P REPRINT
Zilong Wang1, Nan Chen1, Luna K. Qiu1, Ling Yue2,3,
Geli Guo1,Yang Ou1,Shiqi Jiang1,Yuqing Yang1,Lili Qiu1
1Microsoft Research Asia, Shanghai,China
2Department of Geriatric Psychiatry, Shanghai Mental Health Center,
Shanghai Jiao Tong University School of Medicine, Shanghai, China
3Alzheimer’s Disease and Related Disorders Center,
Shanghai Jiao Tong University, Shanghai, China
{wangzilong,nanchen,lunaqiu,v-guobella,yang.ou,shijiang,yuqing.yang,liliqiu}@microsoft.com
bellinthemoon@sjtu.edu.cn
ABSTRACT
In recent years, the rapid aging of the global population has led to an increase in cognitive disorders,
such as Alzheimer’s disease, presenting significant public health challenges. Although no effective
treatments currently exist to reverse Alzheimer’s, prevention and early intervention, including cogni-
tive training, are critical. This report explores the potential of AI chatbots in enhancing personalized
cognitive training. We introduce ReMe, a web-based framework designed to create AI chatbots
that facilitate cognitive training research, specifically targeting episodic memory tasks derived from
personal life logs. By leveraging large language models, ReMe provides enhanced user-friendly,
interactive, and personalized training experiences. Case studies demonstrate ReMe’s effectiveness
in engaging users through life recall and open-ended language puzzles, highlighting its potential to
improve cognitive training design. Despite promising results, further research is needed to validate
training effectiveness through large-scale studies that include cognitive ability evaluations. Overall,
ReMe offers a promising approach to personalized cognitive training, utilizing AI capabilities to meet
the growing demand for non-pharmacological interventions in cognitive health, with future research
aiming to expand its applications and efficacy.
1 Background and Introduction
In recent decades, the global population has been aging at an unprecedented rate, leading to a significant increase in the
elderly population (aged 65 and older), which rose from 8.43% in 1950 to 15.37% in 2023[ 1]. This demographic shift
has contributed to a global increase in cognitive disorders, including Alzheimer’s disease and vascular dementia, which
now represent considerable public health challenges. Alzheimer’s disease, the most prevalent form of dementia, affects
approximately 1 in 9 people aged 65 and older and was the 5th leading cause of death in this age group in 2021[2].
Despite advances in medical research, there are still no effective treatments to reverse Alzheimer’s disease, making
prevention and early intervention critical. From a public health perspective, various interventions have been explored to
delay the onset of cognitive decline, including lifestyle changes such as physical activity, dietary modifications, and
cognitive training programs.
In this context, cognitive training has emerged as a promising non-pharmacological intervention. This report examines
the potential of AI chatbots in enhancing personalized cognitive training for older adults. We propose a chatbot
framework called ReMe, designed to support episodic memory and open-world cognitive training tasks through
personalized interventions.arXiv:2410.19733v1  [cs.AI]  25 Oct 2024"
"FISHNET: Financial Intelligence from Sub-querying, Harmonizing,
Neural-Conditioning, Expert Swarms, and Task Planning
Nicole Cho
J. P. Morgan AI Research
New York, NY, USA
nicole.cho@jpmorgan.comNishan Srishankar
J. P. Morgan AI Research
New York, NY, USA
nishan.srishankar@jpmchase.com
Lucas Cecchi
J. P. Morgan AI Research
New York, NY, USA
lucas.cecchi@jpmchase.comWilliam Watson
J. P. Morgan AI Research
New York, NY, USA
william.watson@jpmchase.com
Abstract
Financial intelligence generation from vast data sources has typi-
cally relied on traditional methods of knowledge-graph construc-
tion or database engineering. Recently, fine-tuned financial domain-
specific Large Language Models (LLMs), have emerged. While these
advancements are promising, limitations such as high inference
costs, hallucinations, and the complexity of concurrently analyz-
ing high-dimensional financial data, emerge. This motivates our
invention FISHNET (Financial Intelligence from Sub-querying, Har-
monizing, Neural-Conditioning, Expert swarming, and Task plan-
ning), an agentic architecture that accomplishes highly complex
analytical tasks for more than 98,000 regulatory filings that vary im-
mensely in terms of semantics, data hierarchy, or format. FISHNET
shows remarkable performance for financial insight generation
(61.8% success rate over 5.0% Routing, 45.6% RAG R-Precision). We
conduct rigorous ablations to empirically prove the success of FISH-
NET, each agent’s importance, and the optimized performance of
assembling all agents. Our modular architecture can be leveraged
for a myriad of use-cases, enabling scalability, flexibility, and data
integrity that are critical for financial tasks.
CCS Concepts
•Applied computing →Economics ;•Information systems
→Information retrieval query processing ;Information ex-
traction .
Keywords
LLM Agents, Swarming, Harmonizing, Planning, Sub-querying
ACM Reference Format:
Nicole Cho, Nishan Srishankar, Lucas Cecchi, and William Watson. 2024.
FISHNET: Financial Intelligence from Sub-querying, Harmonizing, Neural-
Conditioning, Expert Swarms, and Task Planning. In 5th ACM International
Conference on AI in Finance (ICAIF ’24), November 14–17, 2024, Brooklyn, NY,
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
ICAIF ’24, November 14–17, 2024, Brooklyn, NY, USA
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-1081-0/24/11
https://doi.org/10.1145/3677052.3698597
Swarm Intelligence LLM Harmonizer 
FISHNETS (Swarm + Harmonizer) Agent
Agent
Agent Agent Agent Agent
Agent AgentAgent
Agent Agent Agent
Agent
Agent AgentLLM
LLMFigure 1: Traditional Swarm Intelligence (SI) vs. FISHNET: SI
relies on highly capable individual agents collectively work-
ing towards an efficient solution that exceeds the respective
capabilities of those agents. SI typically excludes the pres-
ence of a central entity that harmonizes the agents’ actions.
Recently, a plethora of studies have explored an LLM’s capa-
bility to orchestrate actions. In FISHNET, we combine these
two approaches together; a central harmonizer can orches-
trate while the expert agents also communicate.
USA. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/3677052.
3698597
1 Introduction
Agent-Based Modeling (ABM) in finance has traditionally focused
on simulating the interactions amongst market participants or sim-
ulating economic scenarios [ 21]. In the realm of financial insight
generation, traditional methods of knowledge graph construction
have proliferated the research arena [ 11]. In this context, ABM to
generate financial intelligence from multiple knowledge bases is a
relatively unexplored realm of research for AI in finance. Moreover,arXiv:2410.19727v1  [cs.AI]  25 Oct 2024"
"V ARS: Vision-based Assessment of Risk in
Security Systems
Pranav Gupta, Pratham Gohil, Sridhar S
School of Computing, SRM Institute of Science and Technology, Kattankulathur, Tamil Nadu–603203, India
pm4043@srmist.edu.in, ph7215@srmist.edu.in, sridhars@srmist.edu.in
Abstract —The accurate prediction of danger levels in video
content is critical for enhancing safety and security systems, par-
ticularly in environments where quick and reliable assessments
are essential. In this study, we perform a comparative analysis
of various machine learning and deep learning models to predict
danger ratings in a custom dataset of 100 videos, each containing
50 frames, annotated with human-rated danger scores ranging
from 0 to 10. The danger ratings are further classified into three
categories: no alert (less than 7)and high alert (greater than
equal to 7). Our evaluation covers classical machine learning
models, such as Support Vector Machines, as well as Neural
Networks, and transformer-based models. Model performance is
assessed using standard metrics such as accuracy, F1-score, and
mean absolute error (MAE), and the results are compared to
identify the most robust approach. This research contributes to
developing a more accurate and generalizable danger assessment
framework for video-based risk detection.
Index Terms —danger prediction, alert detection
I. I NTRODUCTION
In recent years, the rise of video content has significantly
increased the demand for robust risk detection systems ca-
pable of accurately assessing dangerous situations in real-
time [1]. The ability to automatically predict the level of
danger in video streams is essential for enhancing safety in
security applications, such as surveillance, public safety, and
autonomous systems [2]. Traditional methods [3] of assessing
video content rely heavily on manual review, which is not
scalable or efficient for large-scale deployments. As such, there
is a growing need for automated systems that can quickly and
accurately predict danger levels based on video data.
The field tends to emphasize ”danger detection” methods
rather than ”danger assessment,” which has limitations in
terms of contextual generalization. Many existing approaches
have a narrow focus, concentrating on specific detection
techniques [4]–[7] or targeting particular types of dangers
[5], [8]–[10], often ignoring the broader contextual elements
essential for precise risk assessment. These non-generalized
methods, especially in temporal visual scenarios, may overlook
crucial factors like the broader danger context, relationships
between objects, and localized actions over time—elements
that are critical when evaluating the risk to humans in a
scene. The widespread reliance on convolutional neural net-
works (CNNs) for danger and risk detection [11]–[16] often
necessitates extensive amounts of labeled data, which can be
labor-intensive to gather but is vital for developing generalized
danger assessment systems using CNNs.In deep learning, understanding how a model perceives
danger requires examining its approach to evaluating risk in a
scene, highlighting that danger assessment can provide deeper
insights than mere detection. Assessing danger in videos
involves identifying hazardous elements and measuring the
risk level they present, requiring more than simple object
recognition or classification. It demands a comprehensive
grasp of the context, interactions, and potential outcomes
unfolding over time.
In this study, we make several key contributions to the field
of video-based danger assessment:
1) Framework Development: We design and implement mul-
tiple machine learning and deep learning frameworks aimed at
predicting danger levels in video content. These frameworks
incorporate classical models like Support Vector Machines
(SVMs) as well as modern deep learning architectures, in-
cluding neural networks and transformer-based models.
2) Integration of Video and Text Embeddings: Our approach
leverages CLIP [17] embeddings for video frames and GPT
[18] embeddings for textual summaries of the video content.
We employ a strategy where embeddings from both modalities
are combined to enhance the accuracy of danger prediction.
4) Comparative Model Analysis: We perform an extensive
comparison of different models using a variety of embed-
dings. These comparisons include neural network-based binary
classification models, SVM-based classifiers, and regression
models using embeddings from CLIP and BERT.
Exploration of Model Architectures: Our study covers the
design and evaluation of various architectures to determine
their effectiveness in accurately predicting danger ratings in
videos, contributing to more generalizable and scalable video-
based risk detection methods.
II. R ELATED WORKS
Hazardous Scene Classification : Hazardous scene clas-
sification involves the automatic identification of potentially
dangerous situations in images or videos, such as accidents,
riots, or assaults. Previous studies have primarily focused on
creating datasets depicting hazardous scenarios [1], [19]–[21]
and developing methods for detecting these scenarios [1], [3].
The distinction between normal and dangerous scenes is often
unclear [21], which can be addressed by evaluating danger
on a standardized scale. This research is vital for enhancing
public safety and preventing accidents.arXiv:2410.19642v1  [cs.AI]  25 Oct 2024"
"Planning-Aware Diffusion Networks for Enhanced Motion Forecasting
in Autonomous Driving
Yunhao Liu∗,1and Hong Ding∗,2and Ziming Zhang2and Huixin Wang3and Jinzhao Liu2and Suyang Xi2,†
1Fudan University
Shanghai, China
Email: 21307130196@m.fudan.edu.cn
2Xiamen University
Fujian, China
{eee2109299, eee2109269@, eee2109297, jrn2109419 }@xmu.edu.my
3Sun Yet-Sen University
Guangdong, China
wanghx73@mail2.sysu.edu.cn
Abstract — Autonomous driving technology has seen significant
advancements, but existing models often fail to fully capture
the complexity of multi-agent environments, where interactions
between dynamic agents are critical. To address this, we
propose the Planning-Integrated Forecasting Model (PIFM),
a novel framework inspired by neural mechanisms governing
decision-making and multi-agent coordination in the brain.
PIFM leverages rich contextual information, integrating road
structures, traffic rules, and the behavior of surrounding
vehicles to improve both the accuracy and interpretability of
predictions. By adopting a diffusion-based architecture, akin to
neural diffusion processes involved in predicting and planning,
PIFM is able to forecast future trajectories of all agents within
a scenario. This architecture enhances model transparency,
as it parallels the brain’s method of dynamically adjusting
predictions based on external stimuli and other agents’ behav-
iors. Extensive experiments validate PIFM’s capacity to provide
interpretable, neuroscience-driven solutions for safer and more
efficient autonomous driving systems, with an extremely low
number of parameters.
I. INTRODUCTION
Autonomous driving has revolutionized the automotive in-
dustry, promising significant improvements in safety and
efficiency [1], [2]. However, the complexity of real-world
driving environments, where multiple dynamic agents such
as vehicles and pedestrians interact, remains a challenge. One
of the key problems that autonomous systems face is accurate
trajectory prediction, which involves forecasting the future
movements of surrounding agents based on their current
states and the environment [3], [4].
Traditionally, motion forecasting models predict the tra-
jectories of individual agents without accounting for the
interdependencies between them. In multi-agent environ-
ments, this approach leads to suboptimal predictions, as
it fails to capture the complex interactions among agents
[5], [6], [7]. To overcome these limitations, recent works
have started incorporating planning information into the
trajectory prediction process [8], [9], allowing systems to
*Corresponding authormake more informed decisions. The integration of planning
data enhances both accuracy and interpretability, but there
remains a need for a comprehensive model that can handle
multi-agent interactions effectively.
Trajectory prediction for autonomous driving has evolved
significantly over the past few years, with various approaches
leveraging multi-modal fusion techniques to improve accu-
racy. Early models relied on historical trajectories and map
data, often concatenating these inputs to provide predic-
tions [10], [11]. More recent works, such as those using
transformer-based architectures, have introduced attention
mechanisms to better integrate these diverse modalities
[12], [13]. These models have improved the predictive ca-
pability of autonomous systems, particularly in scenarios
where agents interact dynamically with their surroundings.
Planning-aware models have also emerged as a promising av-
enue for improving trajectory prediction [8], [9]. PiP [9] and
PRIME [8] explicitly integrate planning information into the
prediction framework, allowing the system to consider future
goals during trajectory generation. However, these methods
often struggle with real-time multi-agent scenarios due to the
computational complexity involved in processing such rich
data. Diffusion-based models have recently been proposed to
address these issues by reducing the computational burden
while maintaining high predictive performance [6].
In this work, we present the Trajectory-Informed Planning
Diffusion (TIP-D) model, which builds on the strengths of
diffusion-based frameworks and integrates planning features
directly into the motion forecasting process. The TIP-D
model incorporates planning information to enhance both
the accuracy and interpretability of trajectory predictions.
Our approach is capable of predicting the trajectories of
multiple agents simultaneously by leveraging cross-attention
mechanisms that dynamically fuse planning features with
environmental data. Moreover, the TIP-D model achieves a
significant reduction in computational complexity, decreas-
ing it by over 80% compared to existing state-of-the-artarXiv:2410.19639v1  [cs.AI]  25 Oct 2024"
"Knowledge Graph Enhanced Language Agents for Recommendation
Taicheng Guo1*, Chaochun Liu2, Hai Wang2, Varun Mannam2, Fang Wang2, Xin Chen2,
Xiangliang Zhang1, Chandan K. Reddy2
1University of Notre Dame2Amazon
{tguo2, xzhang33 }@nd.edu, {chchliu, ihaiwang, mannamvs, fwfang, xcaa, ckreddy }@amazon.com
Abstract
Language agents have recently been used to simulate human
behavior and user-item interactions for recommendation sys-
tems. However, current language agent simulations do not un-
derstand the relationships between users and items, leading to
inaccurate user profiles and ineffective recommendations. In
this work, we explore the utility of Knowledge Graphs (KGs),
which contain extensive and reliable relationships between
users and items, for recommendation. Our key insight is that
the paths in a KG can capture complex relationships between
users and items, eliciting the underlying reasons for user pref-
erences and enriching user profiles. Leveraging this insight,
we propose Knowledge Graph Enhanced Language Agents
(KGLA) , a framework that unifies language agents and KG
for recommendation systems. In the simulated recommenda-
tion scenario, we position the user and item within the KG
and integrate KG paths as natural language descriptions into
the simulation. This allows language agents to interact with
each other and discover sufficient rationale behind their in-
teractions, making the simulation more accurate and aligned
with real-world cases, thus improving recommendation per-
formance. Our experimental results show that KGLA signif-
icantly improves recommendation performance (with a 33%-
95% boost in NDCG@1 among three widely used bench-
marks) compared to the previous best baseline method.
Introduction
Large Language Model (LLM) agents have demonstrated
strong capabilities in conversation and decision-making
across various tasks (Xi et al. 2023; Guo et al. 2024,
2023a). By enabling multiple LLM agents to cooperate, each
agent can have its profile, actions, and behaviors, simulat-
ing diverse human behaviors. Recent works have employed
LLM Agents to simulate real-world recommendation sce-
narios (Zhang et al. 2024b). In these simulations, each agent
represents a user or an item in the recommendation system,
and each agent maintains a memory that records the user
or item’s profiles. The interactions between agents simu-
late user interactions with items, dynamically updating the
agents’ memories to reflect changes in user preferences or
the recognition of unique item features.
While LLM Agents can capture more explainable and ex-
plicit user profiles through text during the recommendation
*Work was done as an intern at Amazon.process, previous work has only considered adding short, ba-
sic descriptions of users and items as prompts during the in-
teractions. As a result, the updated memory for user agents
is nonspecific and general due to a lack of rational informa-
tion about the user’s choice (see Figure 1). With these insuf-
ficient memory profiles, LLMs struggle to identify precise
user preferences and may recommend irrelevant items. In
this work, we analyze the reason for insufficient user agent
and item agent profiles, primarily due to the simulation rely-
ing solely on simple descriptions without rationalizing why
users like or dislike certain items. Consequently, most user
or item agent memories are generated by LLMs based on
limited information. Such profiles heavily rely on the pre-
trained knowledge of LLMs, making them susceptible to
generating generic profiles. User profiling is a critical task
for recommender systems. Hence, addressing the question
of‘how to provide user agents with sufficient information
during Agent simulation to obtain more rational and precise
user profiles?’ remains a crucial but unresolved problem.
To tackle this issue, we aim to leverage a Knowledge
Graph (KG) containing entities with specific meanings to
provide extensive rationales to enhance agent simulation
for recommendation. For example, KG paths can provide
rationales for why a user may like an item (Usermentions−→
featuresdescribe as−→ CD), thus helping to build better user and
item agent profiles. Most of the previous work on using KGs
for recommendation represents the knowledge from a KG in
the form of embeddings and trains a graph neural network
to obtain graph embeddings, which are then concatenated
with user embeddings to represent user profiles. In contrast,
since we use LLM Agents to simulate users and items when
the number of interactions is limited and the user or item
profiles are represented by text, the KG information should
also be represented by text to effectively influence the agent
simulation process.
Based on this motivation, we first identify the informa-
tion from the KG required for recommendation. We for-
mulate this as a ‘ path-to-text ’ problem, where the user and
item can be regarded as nodes in a KG, and paths between
the user and item indicate the rationales for why the user
chooses that item, thus helping to build more precise user
agent memory. There are a few previous works that focus on
leveraging KGs for LLM agents (Jiang et al. 2024; Xu et al.arXiv:2410.19627v1  [cs.AI]  25 Oct 2024"
"Shared Control with Black Box Agents
using Oracle Queries
Inbal Avraham1and Reuth Mirsky1∗
Abstract: Shared control problems involve a robot learning to collaborate with
a human. When learning a shared control policy, short communication between
the agents can often significantly reduce running times and improve the system’s
accuracy. We extend the shared control problem to include the ability to directly
query a cooperating agent. We consider two types of potential responses to a
query, namely oracles: one that can provide the learner with the best action they
should take, even when that action might be myopically wrong, and one with a
bounded knowledge limited to its part of the system. Given this additional in-
formation channel, this work further presents three heuristics for choosing when
to query: reinforcement learning-based, utility-based, and entropy-based. These
heuristics aim to reduce a system’s overall learning cost. Empirical results on two
environments show the benefits of querying to learn a better control policy and the
tradeoffs between the proposed heuristics.
1 Introduction
Many modern systems require a human and a robotic system to work jointly in a coordinated manner.
Surgery and service robots [1, 2], semi-autonomous vehicles [3, 4], and brain-computer interfaces
[5] all involve combining the actions instructed by a human and a robot to form a shared control
system. A shared control, according to Abbink et al. [6], is a system where “Human(s) and robot(s)
are interacting congruently in a perception-action cycle to perform a dynamic task that either of
them could execute individually under ideal circumstances. ” A key challenge in designing such a
system is that the policy of the human agent is hidden from the artificial agent. The human’s hidden
policy can be viewed as a black box . However, existing solutions often assume learners can only
interact with the black box via execution [7, 8]. In many of the mentioned applications, the black
box is a cooperative agent, so a simple question can significantly improve the learning speed and
accuracy of the shared system. This work aims to study the use of queries to improve learning in
a shared control system. This goal is achieved by proposing a new framework for shared control as
visualized in Figure 1. The framework comprises two systems: a black box (representing the human
operator) and a control system (representing the robot).
In the proposed framework, the control can use the assistance of an oracle , which will restrict its
potential action space. With the help of such an oracle, learning an efficient policy is expected to
be easier, which in turn will minimize the number of failed operations or interactions in the shared
system. In this work, we use two oracle types to demonstrate that this expectation is met when the
oracle has full knowledge of the shared system, but in other cases, its advice can actually hinder
the system’s performance. Once queries become part of the robot’s optional learning process, a key
challenge becomes reasoning about when to query the oracle. This paper presents three heuristics
that are designed to increase the accuracy of the learned control policy and to minimize the number
of queries used during training and execution.
The contributions of this work are thus threefold: (1) A formal definition of the shared control
problem with queries; (2) An introduction of two oracle types; and (3) Three heuristics for querying.
These contributions are evaluated on two domains: the first is a set of carefully curated shared
∗1Computer Science Department at Bar Ilan University inbala26@gmail.com, reuthde@gmail.comarXiv:2410.19612v1  [cs.AI]  25 Oct 2024"
"BONGARD IN WONDERLAND :
VISUAL PUZZLES THAT STILL MAKE AI G OMAD?
Antonia Wüst1Tim Tobiasch1Lukas Helff1,2Devendra S. Dhami3
Constantin A. Rothkopf2,4,5Kristian Kersting1,2,4,6
1AI and ML Group, TU Darmstadt;2Hessian Center for AI (hessian.AI)
3Uncertainty in AI Group, TU Eindhoven;4Centre for Cognitive Science, TU Darmstadt
5Institute of Psychology, TU Darmstadt;6German Center for AI (DFKI)
ABSTRACT
Recently, newly developed Vision-Language Models (VLMs), such as OpenAI’s GPT-4o, have
emerged, seemingly demonstrating advanced reasoning capabilities across text and image modalities.
Yet, the depth of these advances in language-guided perception and abstract reasoning remains
underexplored, and it is unclear whether these models can truly live up to their ambitious promises.
To assess the progress and identify shortcomings, we enter the wonderland of Bongard problems, a
set of classical visual reasoning puzzles that require human-like abilities of pattern recognition and
abstract reasoning. While VLMs occasionally succeed in identifying discriminative concepts and
solving some of the problems, they frequently falter, failing to understand and reason about visual
concepts. Surprisingly, even elementary concepts that may seem trivial to humans, such as simple
spirals, pose significant challenges. Moreover, even when asked to explicitly focus on and analyze
these concepts, they continue to falter, suggesting not only a lack of understanding of these elementary
visual concepts but also an inability to generalize to unseen concepts. These observations underscore
the current limitations of VLMs, emphasize that a significant gap remains between human-like visual
reasoning and machine cognition, and highlight the ongoing need for innovation in this area.1
1 Introduction
Visual reasoning, the ability to understand, interpret, and reason about the visual world, is a fundamental aspect of
human intelligence [ 27]. It allows us to navigate our environment, interact with objects, and make sense of complex
visual scenes. In recent years, the field of artificial intelligence (AI) has advanced rapidly toward replicating aspects of
this visual reasoning, with significant focus placed on Vision-Language Models (VLMs) [ 5,24,25]. These models
integrate visual and textual information to generate descriptive content, aiming to mimic how humans comprehend
and reason about the world. Because of their human-like responses, VLMs often create the illusion of possessing
human-like perception and intelligence. However, as recent work shows, VLMs and the Large Language Models (LLM)
on which they are based have dramatic shortcomings in the case of reasoning [ 30] and visual perception [ 12,13,19,34]
or their combination [39, 47, 48].
Bongard problems (BPs), a class of visual puzzles that require the identification of underlying rules based on a
limited set of images, provide a unique and challenging benchmark for assessing visual reasoning abilities in AI
systems [ 4]. Conceived by Russian scientist Mikhail Bongard in 1967, these visual puzzles test cognitive abilities in
pattern recognition and abstract reasoning, posing a formidable challenge even to advanced AI systems [15].
1Code available at https://github.com/ml-research/bongard-in-wonderland .arXiv:2410.19546v1  [cs.AI]  25 Oct 2024"
"EDGE: Enhanced Grounded GUI Understanding with Enriched
Multi-Granularity Synthetic Data
Xuetian Chen
Fudan University
Yangpu Qu, Shanghai Shi, China
23210980105@m.fudan.edu.cnHangcheng Li
Fudan University
Yangpu Qu, Shanghai Shi, China
22307130449@m.fudan.edu.cnJiaqing Liang
Fudan University
Yangpu Qu, Shanghai Shi, China
liangjiaqing@fudan.edu.cn
Sihang Jiang
Fudan University
Yangpu Qu, Shanghai Shi, China
jiangsihang@fudan.edu.cnDeqing Yang
Fudan University
Yangpu Qu, Shanghai Shi, China
yangdeqing@fudan.edu.cn
Abstract
Autonomous agents operating on the graphical user interfaces
(GUIs) of various applications hold immense practical value. Un-
like the large language model (LLM)-based methods which rely
on structured texts and customized backends, the approaches us-
ing large vision-language models (LVLMs) are more intuitive and
adaptable as they can visually perceive and directly interact with
screens, making them indispensable in general scenarios without
text metadata and tailored backends. Given the lack of high-quality
training data for GUI-related tasks in existing work, this paper
aims to enhance the GUI understanding and interacting capabili-
ties of LVLMs through a data-driven approach. We propose EDGE ,
a general data synthesis framework that automatically generates
large-scale, multi-granularity training data from webpages across
the Web. Evaluation results on various GUI and agent benchmarks
demonstrate that the model trained with the dataset generated
through EDGE exhibits superior webpage understanding capabili-
ties, which can then be easily transferred to previously unseen desk-
top and mobile environments. Our approach significantly reduces
the dependence on manual annotations, empowering researchers to
harness the vast public resources available on the Web to advance
their work. Our source code, the dataset and the model are available
at https://anonymous.4open.science/r/EDGE-1CDB.
Keywords
GUI Automation, Large Vison-Language Model, Synthetic Data
1 Introduction
Autonomous interaction with computing devices has long been a
topic of artificial intelligence research [ 7,36]. With the popularity
of personal computers and smartphones, the agents that can auto-
matically interact with graphical user interfaces (GUIs) of various
applications have become a growing focus [ 5,23,33,49]. Recently,
the continuous advance of large language models (LLMs) [ 1,4,52]
makes it increasingly feasible to develop general GUI agents.
Numerous studies [ 15,17,19,21,27,47,50,58,59] have pro-
posed LLM-based GUI agents on web, mobile, and desktop envi-
ronments. These methods avoid visually perceiving GUIs by using
structured texts (e.g., HTML for webpages [ 17,21] and view hi-
erarchy for Android screens [ 47,58]) as input. In addition, they
may require environment-specific backend access to control the
Figure 1: Text-based agents take extracted textual metadata
as input (e.g., HTML) and perform actions through specific
backends (such as browser engines). Vision-based agents di-
rectly read the screen and execute actions like mouse clicks.
systems [ 50,59]. However, given that GUIs are designed for human
interaction through visual reading and basic actions like clicking,
the methods relying on text metadata or customized backend are
neither intuitive nor widely practical. Since GUI formats vary across
different environments, the agents in generalized scenarios must
interact with screens in a human-like manner, without access to
underlying data.
Recognizing this limitation, recent studies [ 13,24,31,44,48,60,
62,66] have turned to the vision-based approaches that employ
large vision-language models (LVLMs) to directly perceive and
interact with screens, as illustrated in Figure 1. To improve the
limited capabilities of LVLMs in grounded GUI understanding and
interacting, where ""grounded"" refers to the ability to precisely lo-
cate elements, these efforts often involve further training on the
GUI-specific datasets with element-level annotations, which are
typically sourced from the Web [ 13,20,24,66]. Specifically, they
drive browser engines to capture the element-level correspondences
between HTML and rendered webpage images, including both posi-
tions and content. This automated process exploits the supervisory
signals inherent on Web, reducing the need for manual annotations.
However, models trained using these methods still exhibit some
drawbacks in real-world scenarios. Firstly, these models generally
underperform on the GUIs with rich visual elements such as imagesarXiv:2410.19461v1  [cs.AI]  25 Oct 2024"
"Offline-to-Online Multi-Agent Reinforcement Learning with
Offline Value Function Memory and Sequential Exploration
Hai Zhong
IIIS, Tsinghua University
Beijing, China
zhongh22@mails.tsinghua.edu.cnXun Wang
IIIS, Tsinghua University
Beijing, China
wang-x24@mails.tsinghua.edu.cn
Zhuoran Li
IIIS, Tsinghua University
Beijing, China
lizr20@mails.tsinghua.edu.cnLongbo Huang
IIIS, Tsinghua University
Beijing, China
longbohuang@tsinghua.edu.cn
ABSTRACT
Offline-to-Online Reinforcement Learning has emerged as a pow-
erful paradigm, leveraging offline data for initialization and on-
line fine-tuning to enhance both sample efficiency and perfor-
mance. However, most existing research has focused on single-
agent settings, with limited exploration of the multi-agent exten-
sion, i.e., Offline-to-Online Multi-Agent Reinforcement Learning
(O2O MARL). In O2O MARL, two critical challenges become more
prominent as the number of agents increases: (i) the risk of un-
learning pre-trained Q-values due to distributional shifts during
the transition from offline-to-online phases, and (ii) the difficulty
of efficient exploration in the large joint state-action space. To
tackle these challenges, we propose a novel O2O MARL framework
called Offline Value Function Memory with Sequential Explo-
ration (OVMSE) . First, we introduce the Offline Value Function
Memory (OVM) mechanism to compute target Q-values, preserv-
ing knowledge gained during offline training, ensuring smoother
transitions, and enabling efficient fine-tuning. Second, we propose
a decentralized Sequential Exploration (SE) strategy tailored
for O2O MARL, which effectively utilizes the pre-trained offline
policy for exploration, thereby significantly reducing the joint state-
action space to be explored. Extensive experiments on the StarCraft
Multi-Agent Challenge (SMAC) demonstrate that OVMSE signifi-
cantly outperforms existing baselines, achieving superior sample
efficiency and overall performance.
KEYWORDS
Multi-Agent Reinforcement Learning, Offline-to-Online Reinforce-
ment Learning
1 INTRODUCTION
Multi-Agent Reinforcement Learning (MARL) has achieved remark-
able success across various domains, including mastering complex
video games [ 1,3,28], optimizing warehouse logistics [ 25], enabling
robotic soccer [ 17], and performing bi-manual dexterous manip-
ulation [ 6]. However, these successes often come at the cost of
low sample efficiency and high computational overhead, as MARL
algorithms must explore a joint state-action space that grows ex-
ponentially with the number of agents. A promising approach to
alleviate this computational burden is Offline-to-Online (O2O) Re-
inforcement Learning (RL). In recent years, O2O RL has achievedsignificant progress [ 10,14,15,19,29,32–34], leveraging advances
in Offline RL by utilizing offline datasets [ 7,8] to provide strong
initial policies and pre-trained value functions. By learning a high-
quality pre-trained policy from offline data, single-agent O2O RL
can significantly reduce the need for extensive exploration and
further enhance performance through interaction during the online
phase. However, existing results primarily focus on single-agent
scenarios, and the important O2O MARL setting has received only
very limited attention, e.g., [12].
O2O MARL faces two major challenges. First, exploration during
the transition from the offline-to-online phase introduces a distri-
butional shift, that can result in significant unlearning of the pre-
trained Q-values in the early stages of online learning. To demon-
strate this phenomenon, we conducted an experiment analyzing
how pre-trained Q-values evolve during online fine-tuning. We col-
lected test trajectories for the offline pre-trained agent interacting
with the environment and stored them into a replay buffer. During
the online phase, we evaluated how online value estimations for
these samples evolved. As shown in Figure 1, Multi-agent Conser-
vative Q-Learning (MACQL) [ 13], Multi-agent Cal-QL (MACal-QL)
[19], and the action proposal method from [ 9] exhibit clear signs of
unlearning, as the fine-tuned Q-values exhibit a rapid drop during
the initial stage of online learning. This suggests that the algorithms
have forgotten the previously learned optimal actions. This unlearn-
ing behavior hinders the efficiency of online fine-tuning, as the
policy has to relearn knowledge that was already acquired during
the offline phase. In contrast, our proposed algorithm OVMSE pre-
serves the offline knowledge and achieves fast online fine-tuning.
Second, efficient exploration is critical for O2O MARL. Explo-
ration at the initial stage of online learning can lead to overestima-
tion of Q-values for unseen state-action pairs. While this overesti-
mation is necessary for agents to experiment with new actions and
improve upon the offline pre-trained policy, inefficient exploration
may result in assigning high Q-values to sub-optimal actions, requir-
ing significant trial and error to correct. This issue is particularly
pronounced in multi-agent systems, where the joint state-action
space grows exponentially with the number of agents. However, un-
like training from scratch, O2O MARL benefits from a pre-trained
offline policy, which provides a much stronger starting point than
a randomly initialized policy. Therefore, an effective exploration
strategy for O2O MARL needs to focus on exploring more efficientlyarXiv:2410.19450v1  [cs.AI]  25 Oct 2024"
"1
Expose Before You Defend: Unifying and
Enhancing Backdoor Defenses via Exposed Models
Yige Li, Hanxun Huang, Jiaming Zhang, Xingjun Ma, and Yu-Gang Jiang, Fellow, IEEE
Abstract —Backdoor attacks covertly implant triggers into
deep neural networks (DNNs) by poisoning a small portion
of the training data with pre-designed backdoor triggers. This
vulnerability is exacerbated in the era of large models, where
extensive (pre-)training on web-crawled datasets is susceptible to
compromise. In this paper, we introduce a novel two-step defense
framework named Expose Before You Defend (EBYD) . EBYD
unifies existing backdoor defense methods into a comprehensive
defense system with enhanced performance. Specifically, EBYD
first exposes the backdoor functionality in the backdoored model
through a model preprocessing step called backdoor exposure , and
then applies detection and removal methods to the exposed model
to identify and eliminate the backdoor features. In the first step
of backdoor exposure, we propose a novel technique called Clean
Unlearning (CUL), which proactively unlearns clean features
from the backdoored model to reveal the hidden backdoor
features. We also explore various model editing/modification
techniques for backdoor exposure, including fine-tuning, model
sparsification, and weight perturbation. Using EBYD, we conduct
extensive experiments on 10 image attacks and 6 text attacks
across 2 vision datasets (CIFAR-10 and an ImageNet subset)
and 4 language datasets (SST-2, IMDB, Twitter, and AG’s
News). The results demonstrate the importance of backdoor
exposure for backdoor defense, showing that the exposed models
can significantly benefit a range of downstream defense tasks,
including backdoor label detection, backdoor trigger recovery,
backdoor model detection, and backdoor removal. More im-
portantly, with backdoor exposure, our EBYD framework can
effectively integrate existing backdoor defense methods into a
comprehensive and unified defense system. We hope our work
could inspire more research in developing advanced defense
frameworks with exposed models. Our code is available at
https://github.com/bboylyg/Expose-Before-You-Defend.
Index Terms —Deep Neural Networks, Backdoor Exposure,
Backdoor Defense, Clean Unlearning
I. I NTRODUCTION
Deep neural networks (DNNs) trained on large-scale
datasets have demonstrated remarkable performance in ad-
dressing complex real-world problems across various domains,
including computer vision (CV) [1], [2] and natural language
processing (NLP) [3], [4]. However, recent studies have shown
that DNNs are vulnerable to backdoor attacks [5], [6], which
Yige Li, Xingjun Ma, and Yu-Gang Jiang are with the Shanghai Key Lab
of Intell. Info. Processing, School of CS, Fudan University, Shanghai, China
(e-mail: xdliyige@gmail.com, xingjunma@fudan.edu.cn, ygj@fudan.edu.cn).
Hanxun Huang is with the School of Computing and Information Systems,
the University of Melbourne, Australia (e-mail: hanxun@unimelb.edu.au).
Jiaming Zhang is with the Department of Computer Science and Engineer-
ing, Hong Kong University of Science and Technology, Hong Kong, China
(e-mail:jmzhang@ust.hk)
Work done during Yige’s internship at Fudan University.
Corresponding Author: Xingjun Mainsert malicious triggers into the model parameters to com-
promise its test-time predictions. Specifically, these attacks
establish a covert correlation between a predefined trigger
pattern and an adversary-specified target label by poisoning
a small subset of the training data. A backdoored model
maintains normal performance on clean inputs but consis-
tently misclassifies inputs containing the trigger pattern to
the target label. Importantly, backdoor attacks are not limited
to a specific domain; they can compromise both vision and
language models. For instance, in the image domain, attackers
may manipulate a few pixels or embed specific patterns,
while in the text domain, they might incorporate particular
words or syntactic structures to trigger malicious behavior.
With the proliferation and accessibility of pre-trained vision
and language models from platforms like Hugging Face [7],
ensuring the secure and backdoor-free deployment of these
models in downstream applications has become increasingly
critical.
Existing defense methods against backdoor attacks can be
broadly categorized into two types: detection methods and
removal methods . Detection methods identify the existence
of a backdoor attack (i.e., trigger) in a trained model (a
task known as backdoor model detection ) or in a training/test
sample (a task known as backdoor sample detection ). Both
tasks involve inverting the trigger pattern used by the attack
and identifying the targeted class of the attacker [5], [8], [9].
Arguably, the ultimate goal of backdoor defense is to com-
pletely eliminate the backdoor trigger from a compromised
model. This objective lies at the core of backdoor removal
methods using techniques such as fine-tuning, pruning [10],
[11], or knowledge distillation [12].
While both backdoor detection and removal methods have
shown promising results, they have been applied indepen-
dently, without benefiting from each other. For example, trig-
ger inversion methods often struggle to identify the backdoor
class and thus have to assume it is known to the defender,
while backdoor removal methods cannot pinpoint the exact
trigger pattern and backdoor class. Moreover, both types of
methods exhibit performance limitations against several ad-
vanced attacks. To date, a unified defense framework capable
of effectively detecting and removing all types of backdoor at-
tacks remains absent from the current literature. Additionally,
none of the existing defense techniques have demonstrated
effectiveness against both image and text backdoor attacks.
“A known enemy is easier to defeat. ”
—Ancient Wisdom
In this work, we aim to address the limitations of existingarXiv:2410.19427v1  [cs.AI]  25 Oct 2024"
"Learning Neural Strategy-Proof Matching Mechanism from Examples
Ryota Maruo, Koh Takeuchi, Hisashi Kashima
Kyoto University
mryota@ml.ist.i.kyoto-u.ac.jp, takeuchi@i.kyoto-u.ac.jp, kashima@i.kyoto-u.ac.jp
Abstract
Designing effective two-sided matching mechanisms is a ma-
jor problem in mechanism design, and the goodness of match-
ing cannot always be formulated. The existing work ad-
dresses this issue by searching over a parameterized fam-
ily of mechanisms with certain properties by learning to fit
a human-crafted dataset containing examples of preference
profiles and matching results. However, this approach does
not consider a strategy-proof mechanism, implicitly assumes
the number of agents to be a constant, and does not consider
the public contextual information of the agents. In this paper,
we propose a new parametric family of strategy-proof match-
ing mechanisms by extending the serial dictatorship (SD). We
develop a novel attention-based neural network called Neu-
ralSD, which can learn a strategy-proof mechanism from a
human-crafted dataset containing public contextual informa-
tion. NeuralSD is constructed by tensor operations that make
SD differentiable and learns a parameterized mechanism by
estimating an order of SD from the contextual information.
We conducted experiments to learn a strategy-proof matching
from matching examples with different numbers of agents.
We demonstrated that our method shows the superiority of
learning with context-awareness over a baseline in terms of
regression performance and other metrics.
1 Introduction
Two-sided matching is a major problem in mechanism de-
sign and has been applied to various real-world applica-
tions, such as labor markets for medical interns and residents
(Roth 1984), assignments in schools (Abdulkadiro ˘glu and
S¨onmez 2003), and matchings between firms and banks in
loan markets (Chen and Song 2013). For example in school
choice, a public center called a mechanism designer devel-
ops a matching mechanism to form pairs of students and
schools based on their reported preferences for each other.
Recently, the public contextual information of agents has
played a crucial role for better matching outcomes. For ex-
ample, in affirmative action for minorities, race and income
status are used to achieve a fair matching in school choice
in Brazil (Ayg ¨un and B ´o 2021; Dur, Pathak, and S ¨onmez
2020). In matching children with daycare centers, family
structures and parents’ health are employed as context to re-
duce the daily burden on families in Japan (Sun et al. 2023).
Without the contextual information, we equally treat agents
and thus cannot obtain better outcomes for social goods.Mechanism designers are usually assumed to be able to
formulate an objective that represent goodness of a match-
ing. Stability is a standard social objective, whereby that no
pair of agents prefers being rematching between them (Roth
and Sotomayor 1990). Strategy-proofness is another im-
portant social objective, meaning that no agent can bene-
fit from misrepresenting their preferences. This objective is
crucial in various applications, such as the affirmative ac-
tion (Ayg ¨un and B ´o 2021) because neither majorities nor
minorities get benefit than their contexts. Serial dictatorship
(SD) is a mechanism that satisfies strategy-proofness (Ab-
dulkadiro ˘glu and S ¨onmez 1998; Satterthwaite and Sonnen-
schein 1981), where agents are ordered sequentially and
each agent either selects their most preferred unmatched
partner or remains single if it is more desirable.
Clearly defining social objectives is not always possible
in the real world. For example, typical ethical requirements
for matching are too complex to represent as mathematical
formulas (Li 2017; S ¨onmez and ¨Unver 2022). In such cases,
mechanism designers have managed to obtain matching us-
ing implicit and empirical rules to achieve desired objectives
given contexts. No matter how good the matching they have
achieved by coordinating stakeholders, we cannot reproduce
it because no explicit rule exists. Thus, an alternative ap-
proach is required that learns an implicit rule from existing
matching records and automatically builds a mechanism.
Narasimhan, Agarwal, and Parkes (2016) proposed a
novel framework for learning desirable matching mecha-
nisms from a set of matching records. They considered
reported preferences and matching results as inputs and
outputs for a learning task, assuming that the mecha-
nism designer’s implicit objective is encoded in the records
of matching results. Then, they introduced a parameter-
ized family of matching mechanisms that inherently sat-
isfy stability, and proposed a method for finding a mech-
anism by optimizing the parameters with a given dataset
using structured support vector machines (Tsochantaridis
et al. 2005). Although novel, there are three problems with
this framework. First, there is no parameterized family of
strategy-proof matching mechanisms, even for the learned
stable mechanism, because no stable matching mechanism
is entirely strategy-proof (Roth 1982; Roth and Sotomayor
1990). Second, their framework implicitly assumes the num-
ber of agents to be constant and cannot be applied to varyingarXiv:2410.19384v1  [cs.AI]  25 Oct 2024"
"Engineering Trustworthy AI: A Developer Guide
for Empirical Risk Minimization
Diana Pfau and Alexander Jung
Department of Computer Science, Aalto University, Espoo, Finland
Abstract —AI systems increasingly shape critical decisions
across personal and societal domains. While empirical risk
minimization (ERM) drives much of the AI’s success, it typically
prioritizes accuracy over trustworthiness, often resulting in
biases, opacity, and other adverse effects. This paper discusses
how key requirements for trustworthy AI can be translated into
design choices for the components of ERM. We hope to provide
actionable guidance for building AI systems that meet emerging
standards for trustworthiness of AI.
Index Terms —Trustworthy AI, Empirical Risk Minimization,
AI Ethics, Responsible AI Design.
I. I NTRODUCTION
Artificial intelligence (AI) has become integral to our daily
lives, influencing aspects such as job searches, housing, and
relationships through AI-powered platforms [1], [2]. Most
current AI systems employ machine learning (ML) to train
personalized models for users. These trained models provide
tailored predictions on interests like job offers, dating, and
music videos [3]. The availability of tailored (personalized)
predictions is instrumental for many applications. As a point
in case, the use of personalized diagnosis and treatment can
significantly improve healthcare [4].
A. Evidence for Harmful AI
Despite the usefulness of ML applications, there is increas-
ing evidence for their potentially harmful effects:
•Impact on Democratic Processes. Social media plat-
forms use ML in the form of recommender systems
to select (or suppress) information presented to a user
[5]. These recommendation systems can (be exploited to)
amplify sensationalist and divisive content which, in turn,
can deepen polarization and the fragmentation of public
sphere into filter bubbles [6], [7]. There is also evidence
for the exploitation of these effects in order to influence
core democratic processes such as elections [8], [9].
•Autopilot Crashes. AI based control of vehicles, which
facilitates semi-autonomous driving, has been associated
with several notable accidents. In some instances, the sys-
tem did not accurately detect obstacles or misinterpreted
road conditions [10], [11]. AI-based autopilots might
also reduce driver engagement and, in turn, awareness
for dangerous situations that require human intervention
[12]. This case illustrates the importance of requiring
This work was supported by Research Council of Finland grant nr. 363624 ,
349965 and331197 .AI systems to be transparent about their operation and
limitations [13].
•The Cambridge Analytica Scandal. Cambridge Ana-
lytica was a British political consulting and data analysis
firm that became widely known for its controversial use
of personal data. The company accessed vast amounts
of personal data from Facebook users without explicit
permission, violating privacy rights and data protection
regulations [14]–[16]. Cambridge Analytica used the data
to create detailed profiles of individuals’ personalities, po-
litical views, and behavioural traits, which were then used
to micro-target individuals with tailored political ads [17].
The firm was involved in several high-profile political
campaigns, including Donald Trump’s 2016 presidential
campaign and the Leave.EU campaign for Brexit, using
data-driven strategies to sway public opinion [18]. The
Cambridge Analytica scandal sparked a global conversa-
tion about data privacy and the impact of AI systems on
societal wellbeing and core democratic processes [19].
•COMPAS Recidivism Prediction Algorithm. A study
found that the COMPAS algorithm, used in the U.S.
justice system to predict recidivism, disproportionately
predicted African-American and female defendants at
higher risk compared to white male defendants [20], [21].
This finding raised concerns about the unbiased-ness and
fairness of the COMPAS algorithm [22].
•Uighur minority in China. Reports suggest that China
has utilized facial recognition technology to specifically
identify individuals with Uighur characteristics, targeting
the Muslim minority group [23], [24]. The targeting of
the Uighur minority by the state is well-documented,
with numerous accounts indicating the presence of what
are referred to as ’re-education centers,’ described by
human rights organizations as high-security detention
camps [25], [26]. The use of facial recognition technology
to target a specific ethnic group highlights fundamental
concerns about harmful effects or misuse of AI systems.
In particular, AI-systems must adhere to ethical principles
and respect human rights including privacy and wellbeing
(individual and on societal level).
B. The Need to Regulate AI
The use of AI is already regulated by existing legal frame-
works. Indeed, any smartphone app that uses AI must conform
to existing consumer protection law [27], [28]. However, thesearXiv:2410.19361v1  [cs.AI]  25 Oct 2024"
"1
LArctan-SKAN: Simple and Efficient
Single-Parameterized Kolmogorov–Arnold
Networks using Learnable Trigonometric
Function
Zhijie Chen and Xinglin Zhang
Abstract
This paper proposes a novel approach for designing Single-Parameterized Kol-
mogorov–Arnold Networks (SKAN) by utilizing a Single-Parameterized Function (SFunc)
constructed from trigonometric functions. Three new SKAN variants are developed: LSin-
SKAN, LCos-SKAN, and LArctan-SKAN. Experimental validation on the MNIST dataset
demonstrates that LArctan-SKAN excels in both accuracy and computational efficiency.
Specifically, LArctan-SKAN significantly improves test set accuracy over existing mod-
els, outperforming all pure KAN variants compared, including FourierKAN, LSS-SKAN,
and Spl-KAN. It also surpasses mixed MLP-based models such as MLP+rKAN and
MLP+fKAN in accuracy. Furthermore, LArctan-SKAN exhibits remarkable computational
efficiency, with a training speed increase of 535.01% and 49.55% compared to MLP+rKAN
and MLP+fKAN, respectively. These results confirm the effectiveness and potential of
SKANs constructed with trigonometric functions. The experiment code is available at
https://github.com/chikkkit/LArctan-SKAN.
Index Terms
Kolmogorov–Arnold Networks, KAN, Single-Parameterized Function, Neural Networks,
SKAN, LArctan-SKAN
Zhijie Chen and Xinglin Zhang are with School of Computer Science and Engineering, South China University of
Technology, Guangzhou, China. Emails: zhijiechencs@gmail.com, zhxlinse@gmail.com.arXiv:2410.19360v1  [cs.AI]  25 Oct 2024"
"APRESCRIPTIVE THEORY FOR BRAIN -LIKE INFERENCE
Hadi Vafaii∗Dekel Galor∗Jacob L. Yates
Redwood Center for Theoretical Neuroscience, Vision Science, UC Berkeley
{vafaii, galor, yates }@berkeley.edu
ABSTRACT
The Evidence Lower Bound (ELBO) is a widely used objective for training deep generative models,
such as Variational Autoencoders (V AEs). In the neuroscience literature, an identical objective is
known as the variational free energy, hinting at a potential unified framework for brain function and
machine learning. Despite its utility in interpreting generative models, including diffusion models,
ELBO maximization is often seen as too broad to offer prescriptive guidance for specific architectures
in neuroscience or machine learning. In this work, we show that maximizing ELBO under Poisson
assumptions for general sequence data leads to a spiking neural network that performs Bayesian
posterior inference through its membrane potential dynamics. The resulting model, the iterative
Poisson V AE (i P-V AE), has a closer connection to biological neurons than previous brain-inspired
predictive coding models based on Gaussian assumptions. Compared to amortized and iterative V AEs,
iP-V AE learns sparser representations and exhibits superior generalization to out-of-distribution
samples. These findings suggest that optimizing ELBO, combined with Poisson assumptions, provides
a solid foundation for developing prescriptive theories in NeuroAI.
Keywords ELBO, iterative inference, variational inference, sparse coding, out-of-distribution generalization
1 Introduction
Optimizing the Evidence Lower Bound (ELBO) serves as a unifying objective for training deep generative models
(Hinton et al., 1995; Dayan et al., 1995; Kingma & Welling, 2014; Rezende et al., 2014; Luo, 2022). Even when
models don’t explicitly reference ELBO, they’re often optimizing objectives closely related to it (Luo, 2022; Kingma &
Gao, 2023). This is directly paralleled by the Free Energy Principle (FEP) in neuroscience, which absorbs previous
theoretical frameworks like Predictive Coding, Bayesian Brain, and Active Learning (Friston, 2005, 2009, 2010). FEP
states that a single objective, the minimization of variational free energy, is all that is needed. Because this is equivalent
to maximizing ELBO, it suggests a powerful unifying theoretical framework for neuroscience and machine learning
(Friston, 2010).
However, in many ways, Free Energy (and by proxy, ELBO) is too general to be useful as a theory (Gershman, 2019;
Andrews, 2021). In practice, the specific implementations of FEP predictive coding have been difficult to map directly
onto neural circuits (Millidge et al., 2021a, 2022), struggling with negative rates and prediction signals that have
not been observed empirically (Walsh et al., 2020; Millidge et al., 2022). Similarly, in machine learning, it is often
discovered after the fact that a new objective is actually ELBO maximization (or KL minimization; Hobson (1969))
masquerading as something else (Kingma & Gao, 2023)—and not the other way around. If ELBO is “all you need,”
then why is ELBO not prescriptive?
One possibility, at least in neuroscience, is that ELBO’s lack of prescriptive theory results from incorrect approximating
distributions. In fact, most of the difficulty mapping predictive coding onto neural circuits has to do with terms that
result from the Gaussian assumption (Millidge et al., 2022). In contrast, biological neurons are largely modeled as
conditionally Poisson (Goris et al., 2014).
Recent work provides a potential prescriptive route: replacing Gaussians with Poisson distributions. To this end, Vafaii
et al. (2024) introduced a reparameterization algorithm for training Poisson Variational Autoencoders ( P-V AE). They
observed that replacing Gaussians in ELBO reduces to an amortized version of sparse coding, an influential model
inspired by the brain that captures many features of the selectivity in early visual cortex (Olshausen & Field, 1996,
2004). P-V AE learns sparse representations, avoids posterior collapse, and performs better on downstream classification
tasks. However, the authors identified a large amortization gap in P-V AE (Vafaii et al., 2024), adding to a growing body
of work that highlights limitations of amortized inference (Cremer et al., 2018; Kim & Pavlovic, 2021). A potentialarXiv:2410.19315v1  [cs.AI]  25 Oct 2024"
" 1 Autonomous Building Cyber-Physical Systems Using Decentralized Autonomous Organizations, Digital Twins, and Large Language Model  Reachsak Ly,1 Alireza Shojaei, Ph.D.2  1 Ph.D. Student, Myers-Lawson School of Construction, Virginia Polytechnic Institute and State University. ORCID: https://orcid.org/0000-0003-0332-1312. Email: reachsak@vt.edu  2 Assistant Professor, Myers-Lawson School of Construction, Virginia Polytechnic Institute and State University, (corresponding author) ORCID: https://orcid.org/0000-0003-3970-0541.  Email: shojaei@vt.edu   Abstract  Current autonomous building research primarily focuses on energy efficiency and automation. While traditional artificial intelligence has advanced autonomous building research, it often relies on predefined rules and struggles to adapt to complex, evolving building operations. Moreover, the centralized organizational structures of facilities management also hinder transparency in decision-making in the building operation management process, which limits the true building autonomy. Research on decentralized governing and adaptive building infrastructure, which could overcome these challenges, remains relatively unexplored. This paper addresses these limitations by introducing a novel Decentralized Autonomous Building Cyber-Physical System framework that integrates Decentralized Autonomous Organizations (DAOs), Large Language Models (LLMs), and digital twin to create a smart, self-managed, operational, and financially autonomous building infrastructure. This approach aims to enhance adaptability, enable decentralized decision-making, and achieve both operational and financial autonomy in building management. This study develops a full-stack decentralized application (Dapp) to facilitate decentralized governance of building infrastructure. An LLM-based artificial intelligence assistant is developed to provide intuitive human-building interaction for blockchain and building operation management-related tasks and enable autonomous building operation. The framework is validated through prototype implementation in a real-world building, with evaluations focusing on workability, cost, scalability, data security, privacy, and integrity. Six real-world scenarios were tested to evaluate the autonomous building system's workability, including building revenue and expense management, AI-assisted facility control, and autonomous adjustment of building systems.  Results indicate that the prototype successfully executes these operations, confirming the framework’s suitability for developing building infrastructure with decentralized governance and autonomous operation.  1. Introduction Research on autonomous buildings has become a promising frontier in the field of smart and sustainable infrastructure. Autonomous buildings are characterized by their ability to operate independently through self-management, self-sufficiency, and intelligent operation. These buildings leverage a combination of advanced technologies to monitor, control, and automate the system’s operation, reducing reliance on external resources and enhancing sustainability and efficiency.  Existing research on autonomous buildings has primarily concentrated on achieving maximum energy efficiency, net-zero energy status, and ensuring energy self-sufficiency and off-grid performance. These objectives are undoubtedly crucial and have yielded significant advancements in the research on sustainable building operations. While energy autonomy [1] is crucial, it represents just one dimension of the overarching goal of achieving a comprehensive autonomous building infrastructure. To achieve operational autonomy within the building, researchers have further explored the integration of several advanced technologies, including building automation systems, artificial intelligence (AI), machine learning (ML), the Internet of "
"DESIGNING LLM-AGENTS WITH PERSONALITIES 1
Designing LLM-Agents with Personalities: A Psychometric Approach
Muhua Huang1, Xijuan Zhang2, Christopher Soto3, and James Evans1 4
1University of Chicago
2York University
3Colby College
4Santa Fe Institute; Google
Author Note
Correspondence concerning this article should be addressed to Muhua Huang,
Knowledge Lab, the University of Chicago. Contact: muhua@uchicago.edu.arXiv:2410.19238v1  [cs.AI]  25 Oct 2024"
"Integrating Large Language Models with Internet
of Things Applications
Mingyu Zong, Arvin Hekmati, Michael Guastalla, Yiyi Li,
Bhaskar Krishnamachari
Viterbi School of Engineering, University of Southern California, Los
Angeles, California, USA, 90089 .
Contributing authors: mzong@usc.edu; hekmati@usc.edu;
guastall@usc.edu; yiyili@usc.edu; bkrishna@usc.edu;
Abstract
This paper identifies and analyzes applications in which Large Language Mod-
els (LLMs) can make Internet of Things (IoT) networks more intelligent and
responsive through three case studies from critical topics: DDoS attack detection,
macroprogramming over IoT systems, and sensor data processing. Our results
reveal that the GPT model under few-shot learning achieves 87.6% detection
accuracy, whereas the fine-tuned GPT increases the value to 94.9%. Given a
macroprogramming framework, the GPT model is capable of writing scripts using
high-level functions from the framework to handle possible incidents. Moreover,
the GPT model shows efficacy in processing a vast amount of sensor data by
offering fast and high-quality responses, which comprise expected results and
summarized insights. Overall, the model demonstrates its potential to power a
natural language interface. We hope that researchers will find these case studies
inspiring to develop further.
Keywords: IoT, LLM, Cybersecurity, Macroprogramming, Sensor Data Processing
1 Introduction
The Internet of Things (IoT) system is a transformational technology in the modern
age. It integrates a myriad of devices and enables interconnected devices to commu-
nicate and cooperate seamlessly. IoT has been deployed across numerous domains,
including transportation, healthcare, and resource management. By harnessing the
1arXiv:2410.19223v1  [cs.AI]  25 Oct 2024"
"MAP: M ULTI -HUMAN -VALUE ALIGNMENT PALETTE
Xinran Wang
University of Minnesota
wang8740@umn.eduQi Le
University of Minnesota
le000288@umn.eduAmmar Ahmed
University of Minnesota
ahme0599@umn.edu
Enmao Diao
diao em@hotmail.comYi Zhou
IBM Research
yi.zhou@ibm.comNathalie Baracaldo
IBM Research
baracald@us.ibm.com
Jie Ding
University of Minnesota
dingj@umn.eduAli Anwar
University of Minnesota
aanwar@umn.edu
ABSTRACT
Ensuring that generative AI systems align with human values is essential but chal-
lenging, especially when considering multiple human values and their potential
trade-offs. Since human values can be personalized and dynamically change over
time, the desirable levels of value alignment vary across different ethnic groups,
industry sectors, and user cohorts. Within existing frameworks, it is hard to define
human values and align AI systems accordingly across different directions simul-
taneously, such as harmlessness, helpfulness, and positiveness. To address this,
we develop a novel, first-principle approach called Multi-Human-Value Align-
ment Palette (MAP), which navigates the alignment across multiple human values
in a structured and reliable way. MAP formulates the alignment problem as an op-
timization task with user-defined constraints, which define human value targets. It
can be efficiently solved via a primal-dual approach, which determines whether
a user-defined alignment target is achievable and how to achieve it. We conduct
a detailed theoretical analysis of MAP by quantifying the trade-offs between val-
ues, the sensitivity to constraints, the fundamental connection between multi-value
alignment and sequential alignment, and proving that linear weighted rewards are
sufficient for multi-value alignment. Extensive experiments demonstrate MAP’s
ability to align multiple values in a principled manner while delivering strong em-
pirical performance across various tasks.
1 I NTRODUCTION
Recent advancements in artificial intelligence (AI) have highlighted the critical need for aligning
AI systems with human values, a concept known as human value alignment (Griffith et al., 2013;
Arumugam et al., 2019; Gabriel, 2020). The alignment can serve the purpose of generating outcomes
that are better suited for human ethics (Griffith et al., 2013), personalized needs (Kirk et al., 2024),
or reduced harmful content (Bai et al., 2022). This alignment has traditionally been pursued by
adjusting AI behavior to adhere to specific attributes via preference datasets or reward functions.
This process involves finetuning the original model according to the optimization problem:
max
p∈PEx∼D,y∼p(·|x)
R(x, y)−βD KL 
p(· |x)||p0(· |x)
. (1)
Here,Pdenotes the class of all distributions, p0is the distribution that represents the generative
model to align, pis the distribution that represents the aligned model, Ris a reward function that
quantifies the preference level of any given pair of prompt xand generation y,DKLmeasures the
KL-divergence, and β >0is a regularization hyperparameter. This formulation has deep conceptual
roots in the Bayesian decision theoretic framework (Bissiri et al., 2016). Specifically, if we consider
xas observed data and yas a parameter θ, the problem (1) can be expressed as Eθ∼p(·)
logp(x|
1arXiv:2410.19198v1  [cs.AI]  24 Oct 2024"
"Tailored-LLaMA: Optimizing Few-Shot Learning in
Pruned LLaMA Models with Task-Specific Prompts
Danyal Aftaba,*and Steven Davyb
a,bTechnological University Dublin, Ireland
ORCID (Danyal Aftab): https://orcid.org/0009-0006-9825-9134, ORCID (Steven Davy):
https://orcid.org/0000-0002-3300-1152
Abstract. Large language models demonstrate impressive pro-
ficiency in language understanding and generation. Nonetheless,
training these models from scratch, even the least complex billion-
parameter variant demands significant computational resources ren-
dering it economically impractical for many organizations. With
large language models functioning as general-purpose task solvers,
this paper investigates their task-specific fine-tuning. We employ
task-specific datasets and prompts to fine-tune two pruned LLaMA
models having 5 billion and 4 billion parameters. This process uti-
lizes the pre-trained weights and focuses on a subset of weights using
the LoRA method. One challenge in fine-tuning the LLaMA model is
crafting a precise prompt tailored to the specific task. To address this,
we propose a novel approach to fine-tune the LLaMA model under
two primary constraints: task specificity and prompt effectiveness.
Our approach, Tailored LLaMA initially employs structural pruning
to reduce the model sizes from 7B to 5B and 4B parameters. Sub-
sequently, it applies a carefully designed prompt specific to the task
and utilizes the LoRA method to accelerate the fine-tuning process.
Moreover, fine-tuning a model pruned by 50% for less than one hour
restores the mean accuracy of classification tasks to 95.68% at a 20%
compression ratio and to 86.54% at a 50% compression ratio through
few-shot learning with 50 shots. Our validation of Tailored LLaMA
on these two pruned variants demonstrates that even when com-
pressed to 50%, the models maintain over 65% of the baseline model
accuracy in few-shot classification and generation tasks. These find-
ings highlight the efficacy of our tailored approach in maintaining
high performance with significantly reduced model sizes.
1 Introduction
Large language models (LLMs) [31, 36, 40, 41] trained on mas-
sive textual data have demonstrated remarkable proficiency in in-
terpreting complex language-based tasks [4, 6, 45] and generating
text. Consequently, there is a growing interest in developing large-
scale language models such as LLaMA [41], MPT [39], and Falcon
[1] that allow for efficient inference and fine-tuning. These LLMs
are available in various sizes each suitable for specific tasks. How-
ever, training the LLMs from scratch even for the smallest billion-
parameter model requires substantial computational resources which
is economically unfeasible for most organizations.
In this paper, we introduce a novel approach to produce a com-
pressed, task-specific, and efficient LLaMA model [41] by leverag-
∗Corresponding Author. Email: D22129961@mytudublin.ieing the pre-trained weights, while having less training cost compared
to the one training from scratch. Moreover, we use the structure prun-
ing method to accomplish this objective. Pruning is a widely used
method for compressing the task-specific models [17, 21, 22, 24, 47]
eliminating redundant parameters to speed up inference while main-
taining performance. However, pruning the general purpose LLMs
often results in significant performance degradation compared to
original models [15, 27, 38], especially in scenarios where minimal
computational resources are allocated after pruning. In this work, to
expedite the fine-tuning process and increase the efficiency of the
pruned model under limited data we employ the Low-Rank Adapta-
tion (LoRA) [19] method.
In efficiently fine-tuning the pruned LLaMA model, we iden-
tify two primary technical challenges. Firstly, how can we optimize
the adaptive weights of a pruned LLaMA model for a specialized
task like classification, question-answering, and sentiment analysis?
Traditional fine-tuning methods for the sparse LLMs [27, 47] de-
pend on datasets designed for multi-tasking approaches. These ap-
proaches often result in sub-optimal performance for the specific
tasks. Secondly, the selection of appropriate prompts is crucial for at-
taining optimal performance. Figure 1 shows that employing varied
prompts across distinct domains results in inconsistent accuracy lev-
els, whereas training with task-specific prompts consistently yields
higher accuracy. This demonstrates that even after reducing LLMs
with extensive parameters can efficiently adapt to a particular task
when fine-tuned with relevant prompts. Our main contributions are:
•We propose a novel fine-tuning algorithm for a pruned LLaMA
model dubbed targeted task fine-tuning which finetunes a pruned
model to a specified target task
•We devise a prompt evaluation strategy that selects prompts based
on their impact on the task, which enhances the pruned model
accuracy and adaptability. This focused approach along with the
LoRA method accelerates performance improvement.
•We demonstrate the effectiveness of our approach by fine-tuning
the LLaMA model across two pruned variants with parameters
decreased from 7 billion to 5 billion and 4 billion.
Although our experimental focus was on the 7 billion parameter
LLaMA model, the Tailored-LLaMA approach exhibits significant
potential for generalizability and adaptability to LLMs of varying
sizes having fewer parameters than the baseline models.
This paper is organized as follows; Section 2 provides a compre-
hensive overview of related work in structure pruning and fine-tuningarXiv:2410.19185v1  [cs.AI]  24 Oct 2024"
"Can Self Supervision Rejuvenate Similarity-Based Link
Prediction?
Chenhan Zhang
chenhan.zhang@student.uts.edu.au
University of Technology Sydney
Sydney, AustraliaWeiqi Wang
weiqi.wang-2@student.uts.edu.au
University of Technology Sydney
Sydney, AustraliaZhiyi Tian
zhiyi.tian@student.uts.edu.au
University of Technology Sydney
Sydney, Australia
James J.Q. Yu
jqyu@ieee.org
Southern University of Science and
Technology
Shenzhen, ChinaDali Kaafar
dali.kaafar@mq.edu.au
Macquarie University
Sydney, AustraliaAn Liu
anliu@suda.edu.cn
Soochow University
Suzhou, China
Shui Yu
shui.yu@uts.edu.au
University of Technology Sydney
Sydney, Australia
ABSTRACT
Although recent advancements in end-to-end learning-based link
prediction (LP) methods have shown remarkable capabilities, the
significance of traditional similarity-based LP methods persists
in unsupervised scenarios where there are no known link labels.
However, the selection of node features for similarity computa-
tion in similarity-based LP can be challenging. Less informative
node features can result in suboptimal LP performance. To ad-
dress these challenges, we integrate self-supervised graph learning
techniques into similarity-based LP and propose a novel method:
Self-Supervised Similarity-based LP(3SLP ). 3SLP is suitable for the
unsupervised condition of similarity-based LP without the assis-
tance of known link labels. Specifically, 3SLP introduces a dual-view
contrastive node representation learning (DCNRL) with crafted
data augmentation and node representation learning. DCNRL is
dedicated to developing more informative node representations,
replacing the node attributes as inputs in the similarity-based LP
backbone. Extensive experiments over benchmark datasets demon-
strate the salient improvement of 3SLP, outperforming the baseline
of traditional similarity-based LP by up to 21.2%(AUC).
CCS CONCEPTS
•Computing methodologies →Feature selection .
KEYWORDS
Link prediction, Graph neural networks, Self-supervised learning
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
XXX ’25, March XX–XXX, 2024, XXX, XXX
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-XXXX-X/18/06
https://doi.org/XXXXXXX.XXXXXXXACM Reference Format:
Chenhan Zhang, Weiqi Wang, Zhiyi Tian, James J.Q. Yu, Dali Kaafar, An
Liu, and Shui Yu. 2024. Can Self Supervision Rejuvenate Similarity-Based
Link Prediction?. In Proceedings of Make sure to enter the correct conference
title from your rights confirmation emai (XXX ’25). ACM, New York, NY,
USA, 10 pages. https://doi.org/XXXXXXX.XXXXXXX
1 INTRODUCTION
Link prediction (LP) is one of the most intriguing and enduring
challenges in the field of graph mining, which involves predicting
the probability of a link between two unconnected nodes based on
available information in the graph, such as node attributes [ 38]. LP
contributes to a myriad of real-world applications such as friend
recommendation in social networks [ 33], drug-drug interaction
prediction [32], and knowledge graph completion [29].
LP methods can be broadly classified into two categories: similarity-
based LP and learning-based LP. While learning-based LP has
shown strong capabilities, there remains a significant role for similarity-
based LP methods. On the one hand, they heavily rely on known
link information as labels to supervise the learning process [ 41,44].
However, in some realistic applications, link information can be
absent, where we only have an edgeless graph . For example, in the
“cold start” phase of the drug-drug interaction, where link labels
may not be readily available [ 14]. Similarity-based LP assists in
hypothesis generation by suggesting potential interactions derived
from the attributes of drugs, where these label information can be
further utilized in unsupervised learning on the data. On the other
hand, the “black box” nature of widely adopted end-to-end neural
network-based LP methods makes it difficult to gain insights into
what inclusive modules or specific data features are pivotal in de-
termining LP. Comparatively, similarity-based is a transparent and
intuitive method that is more conducive to the analysis of results.
Research Gap. However, similarity-based LP methods are heuris-
tic due to their predefined similarity measures [ 19]. Consequently,
the quality of input node features can significantly impact the final
LP performance. This study reexamines similarity-based LP andarXiv:2410.19183v1  [cs.AI]  24 Oct 2024"
"PDL: A Declarative Prompt Programming Language
MANDANA VAZIRI∗, LOUIS MANDEL∗, CLAUDIO SPIESS†, and MARTIN HIRZEL∗
Large language models (LLMs) have taken the world by storm by making many previously difficult uses of AI
feasible. LLMs are controlled via highly expressive textual prompts and return textual answers. Unfortunately,
this unstructured text as input and output makes LLM-based applications brittle. This motivates the rise of
prompting frameworks, which mediate between LLMs and the external world. However, existing prompting
frameworks either have a high learning curve or take away control over the exact prompts from the developer.
To overcome this dilemma, this paper introduces the Prompt Declaration Language (PDL). PDL is a simple
declarative data-oriented language that puts prompts at the forefront, based on YAML. PDL works well with
many LLM platforms and LLMs. It supports writing interactive applications that call LLMs and tools, and
makes it easy to implement common use-cases such as chatbots, RAG, or agents. We hope PDL will make
prompt programming simpler, less brittle, and more enjoyable.
1 Introduction
Large language models (LLMs) have made great advances, demonstrating the ability to perform
a wide range of useful tasks. As LLMs are controlled via natural-language prompts , prompt en-
gineering has emerged as an ad-hoc approach to improve accuracy [ 31]. Even more capabilities
can be unlocked with prompting patterns such as in-context learning [ 6], chaining multiple LLM
calls [7], retrieval-augmented generation (RAG) [18], tool use [28], program-aided language mod-
els (PAL) [ 10], and agents [ 34]. Unfortunately, while powerful, LLMs remain brittle: they sometimes
hallucinate, or even fail to comply with expected syntax and types.
Prompting frameworks [ 20] make it easier for developers to use LLMs and associated prompting
patterns while ameliorating their brittleness. Some, such as LangChain [ 7] and AutoGen [ 32], do so
via bespoke features for popular patterns such as RAG or agents. Unfortunately, this bespokeness
takes control over basic prompts away from users and forces them to learn many complex framework
features. In contrast, low-level prompting frameworks, such as Guidance [ 23] and LMQL [ 5], provide
more control with syntax and types. Unfortunately, they require users to program in imperative
languages such as Python or TypeScript. At the other end of the spectrum, frameworks such as
DSPy [ 15] and Vieira [ 19] avoid hand-written prompts altogether by automatically generating
them. Unfortunately, this takes away even more control from the developer. The problem thus
becomes how to make LLM programming less brittle while keeping it simple and keeping the
developer in the driver’s seat.
To tackle this problem, we turned to tried-and-true programming language design ideas. The
principle of orthogonality advocates for a small set of simple features that compose in powerful
ways [ 30]. Being orthogonal, or at right angles, here means being irredundant and avoiding
exceptional cases as far as possible. For prompting frameworks, orthogonality is a way to avoid
bespoke features. Next, developers need to struggle less with brittleness if the language checks
types androles [12] to enforce structure by construction. One remaining tension is harder to tackle:
on the one hand, we want to give developers control over the exact prompts, but on the other hand,
we want a simple declarative language. To this end, we settled on a data-oriented language , which
puts prompts at the forefront by intentionally blurring the line between programs (e.g. for chaining
and tools) and data (for prompts). This is inspired by the old idea of code as data [ 21], as well as by
seminal work on programming without tiers [8].
∗IBM Research
†UC DavisarXiv:2410.19135v1  [cs.AI]  24 Oct 2024"
"RSA-Control: A Pragmatics-Grounded Lightweight Controllable Text
Generation Framework
Yifan Wang2,3Vera Demberg1,2,3
1Department of Computer Science
2Department of Language Science and Technology
3Saarland Informatics Campus, Saarland University, Germany
{yifwang,vera}@lst.uni-saarland.de
Abstract
Despite significant advancements in natural lan-
guage generation, controlling language models
to produce texts with desired attributes remains
a formidable challenge. In this work, we intro-
duce RSA-Control, a training-free controllable
text generation framework grounded in prag-
matics. RSA-Control directs the generation
process by recursively reasoning between imag-
inary speakers and listeners, enhancing the like-
lihood that target attributes are correctly inter-
preted by listeners amidst distractors. Addition-
ally, we introduce a self-adjustable rationality
parameter, which allows for automatic adjust-
ment of control strength based on context. Our
experiments, conducted with two task types
and two types of language models, demonstrate
that RSA-Control achieves strong attribute con-
trol while maintaining language fluency and
content consistency. Our code is available at
https://github.com/Ewanwong/RSA-Control.
1 Introduction
Controllable text generation (CTG) focuses on pro-
ducing natural language texts with specified at-
tributes, such as sentiment and readability. This
capability is vital for developing functional and re-
liable natural language generation (NLG) systems.
For instance, dialogue systems must be regulated to
consistently generate responses that are low in tox-
icity and bias (Gehman et al., 2020; Kumar et al.,
2023; Sheng et al., 2021). Similarly, summariza-
tion systems are expected to be able to create cus-
tomized summaries for different users by adjusting
readability (Ribeiro et al., 2023).
Many existing studies in CTG rely on fine-tuning
pre-trained language models (PLMs) on attribute-
specific datasets (Keskar et al., 2019; Gururan-
gan et al., 2020). However, due to the increas-
ing scale of PLMs, fine-tuning them has become
resource-intensive. Decoding-based methods that
navigate the PLM decoding process using guidemodules (Dathathri et al., 2020; Yang and Klein,
2021; Krause et al., 2021; Liu et al., 2021) have
achieved strong attribute control and reduced the
need to fine-tune PLMs, but still require additional
datasets and computational resources for training
the guide modules. Besides, introducing external
components could potentially hurt coherence dur-
ing decoding (Xu et al., 2021). As large-scale
PLMs become more adept at understanding human
instructions (Touvron et al., 2023; Achiam et al.,
2023), prompt-based methods have emerged as a
lightweight way to adapt PLMs to new domains
(Brown et al., 2020; Schick and Schütze, 2021).
Previous research has explored direct prompting
(Mattern et al., 2022) and using auxiliary prompts
(Schick et al., 2021; Leong et al., 2023; Yona et al.,
2023) for CTG. Nonetheless, due to the black-box
nature of PLMs, precise control via prompt-based
methods is still challenging and often leads to un-
expected outputs (Zhang et al., 2023).
In this work, we introduce RSA-Control, a
novel CTG method that bridges decoding-based
and prompt-based paradigms through the computa-
tional pragmatic framework of Rational Speech
Acts (RSA) (Frank and Goodman, 2012). The
RSA framework elucidates the effective and ef-
ficient human communication through a mutual
reasoning process: speakers adjust their utterances
by reasoning about listeners’ perceptions, while
listeners, in turn, infer the speakers’ intentions.
Inspired by RSA’s success in modeling conver-
sational behaviors, our approach explicitly mod-
els the interactions between speaker and listener
modules, enabling a pragmatic speaker to generate
utterances that ensure the accurate perception of
desired attributes by the listeners. As illustrated
in Figure 1, RSA-Control constructs a guide mod-
ule (pragmatic listener L1) using PLMs with auxil-
iary control prompts (literal speaker S0) to achieve
controllable decoding of the pragmatic speaker
S1. By replacing fine-tuned discriminator mod-arXiv:2410.19109v1  [cs.AI]  24 Oct 2024"
"ReasonAgain : Using Extractable Symbolic Programs to Evaluate
Mathematical Reasoning
Xiaodong Yu*1,3Ben Zhou∗1,4Hao Cheng2Dan Roth1
1University of Pennsylvania2Microsoft Research
3AMD4Arizona State University
https://github.com/CogComp/reasoning-eval
Abstract
Existing math datasets evaluate the reasoning
abilities of large language models (LLMs) by
either using the final answer or the intermediate
reasoning steps derived from static examples.
However, the former approach fails to surface
model’s uses of shortcuts and wrong reason-
ing while the later poses challenges in accom-
modating alternative solutions. In this work,
we seek to use symbolic programs as a means
for automated evaluation if a model can con-
sistently produce correct final answers across
various inputs to the program. We begin by
extracting programs for popular math datasets
(GSM8K and MATH) using GPT4-o. For those
executable programs verified using the original
input-output pairs, they are found to encapsu-
late the proper reasoning required to solve the
original text questions. We then prompt GPT4-
o to generate new questions using alternative
input-output pairs based the extracted program.
We apply the resulting datasets to evaluate a
collection of LLMs. In our experiments, we ob-
serve significant accuracy drops using our pro-
posed evaluation compared with original static
examples, suggesting the fragility of math rea-
soning in state-of-the-art LLMs.
1 Introduction
Mathematical reasoning is a fundamental skill es-
sential for numerous complex applications, lead-
ing to a recent growing research effort on advanc-
ing large language models (LLMs) in this area.
Thus, proper evaluation of LLMs’ mathematical
reasoning is crucial. Most previous studies have
primarily evaluated LLMs using static datasets,
such as GSM8K (Cobbe et al., 2021) and MATH
(Hendrycks et al., 2021). Typically, evaluations
focus solely on the final answers, overlooking rea-
soning flaws (Lewkowycz et al., 2022) and poten-
tial data contamination issues. Despite impressive
*Equal Contribution. Work done when authors were PhD
students at UPenn.results, LLMs can reply on shortcuts rather than
true reasoning, displaying high sensitivity to input
tokens (Li et al., 2024b,a). Alternatively, some
works (Sawada et al., 2023; Golovneva et al., 2023)
use model-based techniques to assess the reason-
ing quality, but these can suffer from model biases,
limiting accommodation for alternative solutions.
In this paper, we present a focused study on
evaluating mathematical reasoning which can be
concisely encapsulated by symbolic programs, i.e.,
Python programs. For such cases, we can auto-
matically generate a diverse set of new test cases
(input-output pairs) by varying the valid inputs fed
into the program. Thus, if LLMs truly employ
the appropriate reasoning process (as embodied by
the programs) to solve the original question, they
should also be able to consistently solve all new
test cases. This approach allows us to evaluate the
reasoning quality directly by examining the final
answers, without ruling out alternatives.
To avoid costly manual annotations, we use the
state-of-the-art (SoTA) LLM (GPT4-o) to gener-
ate Python programs for GSM8K and MATH. We
retain only those questions with extractable pro-
grams, which can be automatically validated for
our evaluation. This means the programs can be ex-
ecuted to produce the original gold answers. Upon
manual inspection, 92% and83% of the programs
from GSM8K and MATH genuinely demonstrate
the correct reasoning process required to solve the
original questions. We then prompt GPT4-o to
propose alternative valid inputs based on the ex-
tracted program and the original question. These
inputs are then used to generate new input-output
pairs derived from the program. Finally, GPT4-
o is tasked to update the original question using
these proposed inputs to create new test cases for
evaluation.
Our experiments reveal significant declines in
the performance of SoTA LLMs when evaluated
on our generated data. For example, for ques-arXiv:2410.19056v1  [cs.AI]  24 Oct 2024"
"INFOGENT : An Agent-Based Framework for Web Information Aggregation
Revanth Gangi Reddy*Sagnik Mukherjee*Jeonghwan Kim*Zhenhailong Wang*
Dilek Hakkani-Tur Heng Ji
University of Illinois at Urbana-Champaign
{revanth3,sagnikm3,jk100,wangz3,dilek,hengji}@illinois.edu
Abstract
Despite seemingly performant web agents on
the task-completion benchmarks, most exist-
ing methods evaluate the agents based on a
presupposition: the web navigation task con-
sists of linear sequence of actions with an end
state that marks task completion. In contrast,
our work focuses on web navigation for infor-
mation aggregation , wherein the agent must
explore different websites to gather informa-
tion for a complex query. We consider web
information aggregation from two different per-
spectives: (i) Direct API-driven Access relies
on a text-only view of the Web, leveraging ex-
ternal tools such as Google Search API to nav-
igate the web and a scraper to extract website
contents. (ii) Interactive Visual Access uses
screenshots of the webpages and requires in-
teraction with the browser to navigate and ac-
cess information. Motivated by these diverse
information access settings, we introduce INFO-
GENT1, a novel modular framework for web in-
formation aggregation involving three distinct
components: Navigator, Extractor and Aggre-
gator. Experiments on different information
access settings demonstrate INFOGENT beats
an existing SOTA multi-agent search frame-
work by 7% under Direct API-Driven Access
on FRAMES, and improves over an existing
information-seeking web agent by 4.3% under
Interactive Visual Access on AssistantBench.
1 Introduction
Despite the well-documented success of au-
tonomous web agents (Nakano et al., 2021; Yang
et al., 2023; Zhou et al., 2023; Deng et al., 2024),
the proposed tasks usually perform goal-oriented
web-based tasks involving navigating within a web-
site, interacting with elements like buttons and exe-
cuting complex workflows. (e.g., booking a flight
or scheduling a meeting). However, a critical as-
pect of web-based tasks, information aggregation
*Equal Contribution.
1Code will be available at https://github.com/
gangiswag/infogent .
Figure 1: Overview of INFOGENT under the Direct API
Access andInteractive Visual Access settings: The Nav-
igator uses a tool-based LLM and a browser-controlling
VLM as the web agent respectively, with the Aggrega-
tor’s textual feedback guiding further navigation.
has received relatively less attention. Tasks involv-
ing gathering and presenting relevant data from
diverse web sources are central to many real-world
applications. For instance, humans often visit mul-
tiple websites, using search engines to find relevant
content, and browsing articles, reviews, or forums.
Existing web navigation benchmarks and meth-
ods (Zhou et al., 2023; Deng et al., 2024; Lù et al.,
2024; Zheng et al., 2024b; Koh et al., 2024) pri-
marily focus on linear, goal-oriented tasks, such as
booking a flight from Chicago to London, where
sequential actions lead directly to a predefined
outcome without significant backtracking or ex-
ploration. These approaches address tasks with
clear, predefined goals but overlook the challenge
of aggregating information from multiple sources.
In contrast, open-ended information-seeking tasks,
such as investigating why the Indian education sys-
tem lacks funding and infrastructure, require agents
to explore multiple sources, consider diverse view-
points, and determine when sufficient informationarXiv:2410.19054v1  [cs.AI]  24 Oct 2024"
"GenerativeAIResearchO1 Replication Journey: A Strategic Progress Report – Part 1
Yiwei Qin1,4*Xuefeng Li1,4*Haoyang Zou4*Yixiu Liu1,4*Shijie Xia1,4*
Zhen Huang4Yixin Ye1,4Weizhe Yuan2Hector Liu3Yuanzhi Li3Pengfei Liu1,4†
1Shanghai Jiao Tong University,2New York University,
3MBZUAI,4Generative AI Research Lab (GAIR)
Abstract
This paper introduces a pioneering approach to artificial intelligence research, embodied in our O1
Replication Journey. In response to the announcement of OpenAI’s groundbreaking O1 model, we embark
on a transparent, real-time exploration to replicate its capabilities while reimagining the process of
conducting and communicating AI research. Our methodology addresses critical challenges in modern AI
research, including the insularity of prolonged team-based projects, delayed information sharing, and the
lack of recognition for diverse contributions. By providing comprehensive, real-time documentation of
our replication efforts, including both successes and failures, we aim to foster open science, accelerate
collective advancement, and lay the groundwork for AI-driven scientific discovery . Our research
progress report diverges significantly from traditional research papers, offering continuous updates, full
process transparency, and active community engagement throughout the research journey. Technologically,
we proposed the “ journey learning ” paradigm, which encourages models to learn not just shortcuts,
but the complete exploration process, including trial and error, reflection, and backtracking. With only
327 training samples and without any additional tricks, journey learning outperformed conventional
supervised learning by over 8% on the MATH dataset, demonstrating its extremely powerful potential.
We believe this to be the most crucial component of O1 technology that we have successfully decoded.
We share valuable resources including technical hypotheses and insights, cognitive exploration maps,
custom-developed tools, etc at https://github.com/GAIR-NLP/O1-Journey .
Figure 1: Illustration of our O1 replication journey from September 12 to October 8, 2024. It depicts four key stages:
Initial Assessment, Multi-path Exploration, Iterative Improvement, and Current Results. The journey culminates in
our novel “ journey learning ” approach, which significantly outperforms traditional “shortcut learning” methods.
With only 327 training samples, our journey learning technique surpassed shortcut learning by 8.4% and 8.0%
respectively on the MATH500 (Lightman et al., 2024).
 denotes our Walnut Plan, which aims to revolutionize AI
by developing systems capable of deep scientific thinking, ultimately enabling AI-driven breakthroughs in human
knowledge and discovery.
*Co-first authors
†Corresponding author
1arXiv:2410.18982v1  [cs.AI]  8 Oct 2024"
"Counting Ability of Large Language Models and Impact of Tokenization
Xiang Zhang1∗Juntai Cao1∗Chenyu You2
1University of British Columbia
2Yale University
xzhang23@ualberta.ca, jtcao7@cs.ubc.ca, chenyu.you@yale.edu
Abstract
Transformers, the backbone of modern large
language models (LLMs), face inherent archi-
tectural limitations that impede their reasoning
capabilities. Unlike recurrent networks, Trans-
formers lack recurrent connections, confining
them to constant-depth computation. This re-
striction places them in the complexity class
TC0, making them theoretically incapable of
solving tasks that demand increasingly deep
reasoning as input length grows. Counting,
a fundamental component of many reasoning
tasks, also requires reasoning depth to grow
linearly to be performed inductively. While
previous studies have established the upper lim-
its of counting ability in Transformer-based ex-
pert models (i.e., models specifically trained for
counting tasks), these findings do not directly
extend to general-purpose LLMs due to differ-
ences in reasoning mechanisms. Recent work
has highlighted how Chain of Thought (CoT)
reasoning can help alleviate some of the archi-
tectural limitations of Transformers in count-
ing tasks. However, little attention has been
paid to the role of tokenization in these models.
Unlike expert models that often use character-
level tokenization, LLMs typically rely on byte-
level (BPE) tokenizers, which fundamentally
alters the way reasoning is processed. Our
work investigates the impact of tokenization
on the counting abilities of LLMs, uncovering
substantial performance variations based on in-
put tokenization differences. We provide both
theoretical and experimental analyses, offer-
ing insights into how tokenization choices can
undermine models’ theoretical computability,
thereby inspiring the design of new tokeniza-
tion methods to enhance reasoning in LLMs.
All code, prompts, and experiment logs, API
returns are released on GitHub.
1 Introduction
Counting , a fundamental component of most com-
plex reasoning tasks, has been extensively stud-
∗ ∗Equal contribution.
Figure 1: Experimental results on average counting
accuracy based on different tokenization choices, using
GPT-4o mini. Our approach treats the model as a black-
box, manipulating BPE tokenizers to function differently
through carefully engineered string formats.
ied across various disciplines for decades (Boolos
et al., 2002; Wynn, 1990; De Bruijn, 1964). In
particular, computer scientists have explored the
circuit complexity of counting (Jerrum, 1995), the
needed capabilities of computing machines (Fis-
cher et al., 1968) to perform counting, and how
counting relates to more complex tasks within the
framework of computability theory (Ibarra et al.,
2002). Basic counting (counting from 1ton) re-
quires a depth complexity, the number of sequential
computation steps, that grows with input length
(non-constant). This requirement, grounded in
computability theory (Fischer et al., 1968), serves
as a theoretical constraint for any computational
machine to solve this task, from simple state ma-
chines (Cooper, 2017) to neural networks (LeCun
et al., 2015; Jin et al., 2024). Previous research has
thoroughly examined the computability, the ability
to solve tasks of a given complexity, of different
types of neural networks (Delétang et al., 2022;
Zhang et al., 2024a). Both theoretical (Raghu et al.,
2017) and experimental (Delétang et al., 2022) find-
ings have shown that counting requires an equiv-
alent or higher level of computability than what
LSTM and RNN provide (Delétang et al., 2022),
with the former empirically aligning with the ca-
pabilities of k-counter machines (Delétang et al.,
2022).arXiv:2410.19730v1  [cs.CL]  25 Oct 2024"
"Sparse Decomposition of Graph Neural Networks
Yaochen Hu yaochen.hu@huawei.com
Huawei Noah’s Ark Lab, Montreal, Canada
Mai Zeng mai.zeng@mail.mcgill.ca
McGill University & Mila & ILLS∗, Montreal, Canada
Ge Zhang ge.zhang1@huawei.com
Huawei Noah’s Ark Lab, Toronto, Canada
Pavel Rumiantsev pavel.rumiantsev@mail.mcgill.ca
McGill University & Mila & ILLS, Montreal, Canada
Liheng Ma liheng.ma@mail.mcgill.ca
McGill University & Mila & ILLS, Montreal, Canada
Yingxue Zhang yingxue.zhang@huawei.com
Huawei Noah’s Ark Lab, Toronto, Canada
Mark Coates mark.coates@mcgill.ca
McGill University & Mila & ILLS, Montreal, Canada
Abstract
Graph Neural Networks (GNN) exhibit superior performance in graph representation learn-
ing, but their inference cost can be high, due to an aggregation operation that can require
a memory fetch for a very large number of nodes. This inference cost is the major obstacle
to deploying GNN models with online prediction to reflect the potentially dynamic node
features. To address this, we propose an approach to reduce the number of nodes that are
included during aggregation. We achieve this through a sparse decomposition, learning to
approximate node representations using a weighted sum of linearly transformed features
of a carefully selected subset of nodes within the extended neighbourhood. The approach
achieves linear complexity with respect to the average node degree and the number of layers
in the graph neural network. We introduce an algorithm to compute the optimal parame-
ters for the sparse decomposition, ensuring an accurate approximation of the original GNN
model, and present effective strategies to reduce the training time and improve the learning
process. We demonstrate via extensive experiments that our method outperforms other
baselines designed for inference speedup, achieving significant accuracy gains with compa-
rable inference times for both node classification and spatio-temporal forecasting tasks.
1 Introduction
Graph neural networks (GNN) have demonstrated impressive performance for graph representation learning
(Hamilton et al., 2017; Veličković et al., 2018; Qu et al., 2019; Rampášek et al., 2022). Although there are
numerous designs for GNN models, the essential idea is to represent each node based on its features and its
neighbourhood (Wu et al., 2020; Zhou et al., 2020). The procedure of aggregating features from neighbour
nodes is empirically and theoretically effective (Xu et al., 2019) in representing the graph structures and
blending the features of the nodes. However, deploying GNN models to process large graphs is challenging
∗Mila - Quebec AI Institute and ILLS - International Laboratory on Learning Systems.
1arXiv:2410.19723v1  [cs.LG]  25 Oct 2024"
"2D-DPO: Scaling Direct Preference Optimization
with 2-Dimensional Supervision
Shilong Li∗†, Yancheng He∗, Hui Huang∗, Xingyuan Bu∗‡, Jiaheng Liu,
Hangyu Guo, Weixun Wang, Jihao Gu, Wenbo Su, Bo Zheng
Alibaba Group
zhuli.lsl@taobao.com, xingyuanbu@gmail.com
Abstract
Recent advancements in Direct Preference Op-
timization (DPO) have significantly enhanced
the alignment of Large Language Models
(LLMs) with human preferences, owing to its
simplicity and effectiveness. However, existing
methods typically optimize a scalar score or
ranking reward, thereby overlooking the multi-
dimensional nature of human preferences. In
this work, we propose to extend the prefer-
ence of DPO to two dimensions: segments
andaspects . We first introduce a 2D supervi-
sion dataset called HelpSteer-2D . For the seg-
ment dimension, we divide the response into
sentences and assign scores to each segment.
For the aspect dimension, we meticulously de-
sign several criteria covering the response qual-
ity rubrics. With the 2-dimensional signals as
feedback, we develop a 2D-DPO framework,
decomposing the overall objective into multi-
segment and multi-aspect objectives. Extensive
experiments on popular benchmarks demon-
strate that 2D-DPO performs better than meth-
ods that optimize for scalar or 1-dimensional
preferences.
1 Introduction
Recent advancements in Large Language Models
(LLMs) have shown impressive performance across
a wide range of tasks (Zhao et al., 2023; Bai et al.,
2024; Wu et al., 2024a; Li et al., 2024a). A piv-
otal component in LLM training is Reinforcement
Learning from Human Feedback (RLHF) (Ouyang
et al., 2022; Bai et al., 2022), which aligns LLMs
with human preferences. However, due to its com-
plexity, traditional RLHF often leads to challenges
such as training instability and reward collapse
(Wolf et al., 2023; Song et al., 2023).
∗Equal contribution. ‡Corresponding Author.
†Work done during an internship at Alibaba Group.
Codes and datasets are anonymously at https://
anonymous.4open.science/r/2D-DPO-56E4/ and will be
released to the public once accepted to promote related re-
search.
Overall Score: 2Overall Score: 2
S3:Clarity: 3Correctness: 3Safety: 4…S2:Clarity: 2Correctness: 3Safety: 3…S1:Clarity: 4Correctness: 4Safety: 4…Instruction:  How to prepare for hiking?Response:Choose a suitable route. Lighten gear. Stay alert.
aspectsegment
Holistic Score: 2    Response:Choose a suitable route. Lighten gear.Stay alert.LLMs
LLMs
0-D
2-DFigure 1: An illustrative comparison between vanilla
DPO and 2D-DPO.
Direct Preference Optimization (DPO) (Rafailov
et al., 2023), as a simpler and more effective al-
ternative, has gained considerable attention due
to its ability to bypass the need for explicitly fit-
ting a reward model (Meng et al., 2024; Ethayarajh
et al., 2024). However, most existing DPO-style
approaches rely on scalar scores or rankings and
ignore the multi-dimensional nature of human pref-
erences, resulting in inefficient and imprecise opti-
mization. For instance, a response may be deemed
satisfactory under one aspect such as correctness ,
but falls short in another such as clarity . More-
over, not all segments of a response should be
treated uniformly; even in a preferred response,
there may be segments of inferior quality. This
underscores the need for a more nuanced approach
that recognizes the multi-dimensionality of feed-
back and its critical impact on model training.
In response, some recent works have attempted
to leverage signals that are believed to reflect
the importance of individual segments as reward
scores (Zeng et al., 2024; Chan et al., 2024; Jiang
et al., 2024; Chen et al., 2024). However, these sig-
nals are often derived from statistical features such
as edit distance or confidence estimation, which
can introduce noise and lack interpretability. Other
approaches incorporate multi-objective optimiza-arXiv:2410.19720v1  [cs.CL]  25 Oct 2024"
"Arabic Music Classification and Generation using
Deep Learning
Mohamed Elshaarawy∗, Ashrakat Saeed∗, Mariam Sheta∗, Abdelrahman Said∗, Asem Bakr∗, Omar Bahaa∗, Walid Gomaa∗,†
∗Egypt-Japan University of Science and Technology, Alexandria, Egypt.
†Faculty of Engineering, Alexandria University, Alexandria, Egypt.
{mohamed.elshaarawy, ashrakat.saeed, maryem.abousaad,abdelrahman.said, asem.abdelhamid, omar.bahaa, walid.gomaa }@ejust.edu.eg
Abstract —This paper proposes a machine learning approach
for classifying classical and new Egyptian music by composer
and generating new similar music. The proposed system utilizes
a convolutional neural network (CNN) for classification and a
CNN autoencoder for generation. The dataset used in this project
consists of new and classical Egyptian music pieces composed by
different composers.
To classify the music by composer, each sample is normalized
and transformed into a mel spectrogram. The CNN model is
trained on the dataset using the mel spectrograms as input
features and the composer labels as output classes. The model
achieves 81.4% accuracy in classifying the music by composer,
demonstrating the effectiveness of the proposed approach.
To generate new music similar to the original pieces, a CNN
autoencoder is trained on a similar dataset. The model is trained
to encode the mel spectrograms of the original pieces into a
lower-dimensional latent space and then decode them back into
the original mel spectrogram. The generated music is produced
by sampling from the latent space and decoding the samples back
into mel spectrograms, which are then transformed into audio.
In conclusion, the proposed system provides a promising
approach to classifying and generating classical Egyptian music,
which can be applied in various musical applications, such as
music recommendation systems, music production, and music
education.
Index Terms —Neural Network, MFCC, Convolution, Autoen-
coder, Mel spectrogram, griffin-lim, confusion matrix
I. I NTRODUCTION
This project aims to leverage deep learning techniques
for the classification and generation of Arabic music. The
project encompasses several phases, starting with an extensive
data collection process. A dataset comprising music files
from various genres was assembled. Subsequently, rigorous
pre-processing techniques were employed to optimize the
data for the subsequent modeling stages. The pre-processing
phase involved exploring different approaches to identify the
most effective methods for enhancing model performance.
This involved techniques such as audio normalization, fea-
ture extraction, and data augmentation. By carefully refining
the pre-processing pipeline, the project sought to maximize
the quality and relevance of the input data for subsequent
modeling steps. The next phase involved training and fitting
deep learning models for both music classification and music
generation tasks. The classification models were subjected to
iterative testing and refinement to achieve satisfactory accuracy
levels. For the music generation aspect, the fitted model wasused to generate new musical compositions. Post-processing
techniques were employed to refine and enhance the gener-
ated music, ensuring that it adhered to the desired musical
characteristics and structure. These post-processing steps may
have included melody harmonization, rhythm adjustment, and
tempo normalization. By employing advanced deep learning
techniques and thorough pre-processing methodologies, this
project aims to advance the field of Arabic music classification
and generation. The results obtained from this research can
potentially contribute to the broader domain of music analysis,
synthesis, and creative expression.
II. R ELATED WORK
In recent years, there has been a growing interest in the field
of music classification and generation. Several studies have
explored various techniques and methodologies to address
the challenges associated with this domain. In this section,
the relevant literature on music composer classification, and
generation highlighting the key approaches and achievements
are reviewed.
One notable paper in this field is titled ”Composer Classi-
fication Models for Music-Theory Building” by Herremans,
Martens, and Sorensen (2015)[1]. In their study, the re-
searchers adopted a data-driven approach by extensively ana-
lyzing a large database of existing music. The main objectives
of their work were twofold: first, to develop an automated
system capable of accurately distinguishing between compo-
sitions from different composers, and second, to identify the
significant musical attributes that contribute to this distinction.
A recent Kaggle project by Holst[2] focused on music
composer classification using a machine learning approach.
The project involved building a model to classify musical
compositions based on the underlying composer’s style. Holst
extracted various features from the audio recordings and
employed a classification algorithm to differentiate between
composers. The results obtained in the Kaggle project provide
valuable insights into the application of machine learning
techniques for music composer classification.
The paper titled ”A Comprehensive Survey on Deep Music
Generation”[3] published in the Journal of Artificial Intelli-
gence and Data Analytics is a valuable resource that we uti-
lized to deepen our understanding of Variational Autoencoders
(V AEs) and explore their potential for sound generation. WearXiv:2410.19719v1  [cs.SD]  25 Oct 2024"
"Evolving Neural Networks Reveal Emergent Collective
Behavior from Minimal Agent Interactions
G. S. Y . Giardinia, J. F. Hardy IIa, C. R. da Cunhaa
aSchool of Informatics, Computing, and Cyber-Systems, Northern Arizona University, 1295 S.
Knoles Dr., Flagstaff, AZ, 86011, USA
Abstract
Understanding the mechanisms behind emergent behaviors in multi-agent systems
is critical for advancing fields such as swarm robotics and artificial intelligence.
In this study, we investigate how neural networks evolve to control agents’ be-
havior in a dynamic environment, focusing on the relationship between the net-
work’s complexity and collective behavior patterns. By performing quantitative
and qualitative analyses, we demonstrate that the degree of network non-linearity
correlates with the complexity of emergent behaviors. Simpler behaviors, such
as lane formation and laminar flow, are characterized by more linear network op-
erations, while complex behaviors like swarming and flocking show highly non-
linear neural processing. Moreover, specific environmental parameters, such as
moderate noise, broader field of view, and lower agent density, promote the evo-
lution of non-linear networks that drive richer, more intricate collective behaviors.
These results highlight the importance of tuning evolutionary conditions to induce
desired behaviors in multi-agent systems, offering new pathways for optimizing
coordination in autonomous swarms. Our findings contribute to a deeper under-
standing of how neural mechanisms influence collective dynamics, with implica-
tions for the design of intelligent, self-organizing systems.
Keywords: Collective dynamics, neural networks, evolutionary algorithms,
emergence
Email address: carlo.cunha@nau.edu (C. R. da Cunhaa)
Preprint submitted to Elsevier October 28, 2024arXiv:2410.19718v1  [nlin.AO]  25 Oct 2024"
"Adversarial Environment Design via Regret-Guided
Diffusion Models
Hojun Chung1, Junseo Lee1, Minsoo Kim1, Dohyeong Kim2, and Songhwai Oh1,2,∗
1Interdisciplinary Program in Artificial Intelligence, Seoul National University
2Department of Electrical and Computer Engineering, Seoul National University
{hojun.chung, junseo.lee, minsoo.kim, dohyeong.kim}@rllab.snu.ac.kr,
songhwai@snu.ac.kr
Abstract
Training agents that are robust to environmental changes remains a significant
challenge in deep reinforcement learning (RL). Unsupervised environment design
(UED) has recently emerged to address this issue by generating a set of training
environments tailored to the agent’s capabilities. While prior works demonstrate
that UED has the potential to learn a robust policy, their performance is constrained
by the capabilities of the environment generation. To this end, we propose a novel
UED algorithm, adversarial environment design via regret-guided diffusion models
(ADD). The proposed method guides the diffusion-based environment generator
with the regret of the agent to produce environments that the agent finds challenging
but conducive to further improvement. By exploiting the representation power
of diffusion models, ADD can directly generate adversarial environments while
maintaining the diversity of training environments, enabling the agent to effectively
learn a robust policy. Our experimental results demonstrate that the proposed
method successfully generates an instructive curriculum of environments, outper-
forming UED baselines in zero-shot generalization across novel, out-of-distribution
environments. Project page: https://github.com/rllab-snu.github.io/projects/ADD
1 Introduction
Deep reinforcement learning (RL) has achieved great success in various challenging domains, such
as Atari [ 1], GO [ 2], and real-world robotics tasks [ 3,4]. Despite the progress, the deep RL agent
struggles with the generalization problem; it often fails in unseen environments even with a small
difference from the training environment distribution [ 5,6]. To train well-generalizing policies,
various prior works have used domain randomization (DR) [ 7,8,9], which provides RL agents with
randomly generated environments. While DR enhances the diversity of the training environments,
it requires a large number of trials to generate meaningful structures in high-dimensional domains.
Curriculum reinforcement learning [ 10,11] has been demonstrated to address these issues by pro-
viding instructive sequences of environments. Since manually designing an effective curriculum for
complicated tasks is challenging, prior works [ 12,13] focus on generating curricula that consider the
current agent’s capabilities. Recently, unsupervised environment design (UED, [ 14]) has emerged
as a scalable approach, notable for its advantage of requiring no prior knowledge. UED algorithms
alternate between training the policy and designing training environments that maximize the regret
of the agent. This closed-loop framework ensures the agent learns a minimax regret policy [ 15],
assuming that the two-player game between the agent and the environment generator reaches the
Nash equilibrium.
∗Corresponding author: Songhwai Oh
38th Conference on Neural Information Processing Systems (NeurIPS 2024).arXiv:2410.19715v1  [cs.LG]  25 Oct 2024"
"TimeSuite: Improving MLLMs for Long Video Understanding via Grounded Tuning
TIMESUITE : IMPROVING MLLM S FOR LONG VIDEO
UNDERSTANDING VIA GROUNDED TUNING
Xiangyu Zeng1,2Kunchang Li3,2Chenting Wang6,2Xinhao Li1,2Tianxiang Jiang5,2
Ziang Yan4,2Songze Li7,2Yansong Shi5,2Zhengrong Yue6,2Yi Wang2
Yali Wang3,2Yu Qiao2Limin Wang1,2
1Nanjing University2Shanghai AI Laboratory3SIAT, Chinese Academy of Sciences4Zhejiang University
5University of Science and Technology of China6Shanghai Jiao Tong University7Fudan University
MVBench!ℎ#$%&'()#*+
QVHighlights,'-ℎ.'-ℎ%/)%)0%'#1
Charades-STA2)34#$5.6$#71('1-VideoMME8#1-&'()#*+
Egoschema8#1-&'()#*+
00:1400:24
!""#$%&'("")$*&'(+$!%#,+$))-$'.&'(Time
VideoChat2
!UserTimeChat
VideoChat-TWhat did the man do after throwing away the basketball?
!The man spoke, he threw the ball.
The given query happens between 19.7 -27.6 seconds.After throwing away the basketball, the man started dancing.
/0'(1&-#023At what time in the video does the man dribble the basketball?
!The man dribbles the basketball at the 10th secondof the video.
The given query happens between 10.3 -15.2 seconds.From 13.5 to 24.9 seconds.
4#5""06$)7608'-&'(!""""#""$#%&'(!"")%#%&
!
!
!
!
!
!
!
(Ours)(Baseline)
Figure 1: VideoChat-T demonstrates high performance for both long-form video question an-
swering and temporal grounding. Our TimeSuite presents a collection of new designs to enhance
the long video understanding capability of MLLMs. In particular, We design a new instruction tun-
ing task of Temporal Grounded Caption to explicitly incorporate the grounding supervision into the
traditional QA task. It will implicitly endow the MLLM with ability of correctly attending the visual
segments when generating answers, thus relieving the hallucination risk caused by the LLMs.
ABSTRACT
Multimodal Large Language Models (MLLMs) have demonstrated impressive
performance in short video understanding. However, understanding long-form
videos still remains challenging for MLLMs. This paper proposes TimeSuite , a
collection of new designs to adapt the existing short-form video MLLMs for long
video understanding, including a simple yet efficient framework to process long
video sequence, a high-quality video dataset for grounded tuning of MLLMs, and
a carefully-designed instruction tuning task to explicitly incorporate the ground-
ing supervision in the traditional QA format. Specifically, based on VideoChat, we
propose our long-video MLLM, coined as VideoChat-T, by implementing a token
shuffling to compress long video tokens and introducing Temporal Adaptive Po-
sition Encoding (TAPE) to enhance the temporal awareness of visual representa-
tion. Meanwhile, we introduce the TimePro, a comprehensive grounding-centric
instruction tuning dataset composed of 9 tasks and 349k high-quality grounded
annotations. Notably, we design a new instruction tuning task type, called Tem-
poral Grounded Caption, to perform detailed video descriptions with the corre-
sponding timestamps prediction. This explicit temporal location prediction will
guide MLLM to correctly attend on the visual content when generating descrip-
tion, and thus reduce the hallucination risk caused by the LLMs. Experimental
results demonstrate that our TimeSuite provides a successful solution to enhance
the long video understanding capability of short-form MLLM, achieving improve-
ment of 5.6% and6.8% on the benchmarks of Egoschema and VideoMME, re-
spectively. In addition, VideoChat-T exhibits robust zero-shot temporal grounding
capabilities, significantly outperforming the existing state-of-the-art MLLMs. Af-
ter fine-tuning, it performs on par with the traditional supervised expert models.
1arXiv:2410.19702v1  [cs.CV]  25 Oct 2024"
"1 
 Enhancing Resilience and Scalability in Travel Booking Systems: A 
Microservices Approach to Fault Tolerance, Load Balancing, and 
Service Discovery  
 
Biman Baruaa,b,* [0000 -0001 -5519 -6491 ] and M. Shamim Kaiserb, [0000 -0002 -4604 -5461 ] 
aDepartment of CSE, BGMEA Universitsy of Fashion & Tecnnology, Nishatnagar, Turag, Dhaka -1230, Bangladesh  
bInstitute of Information Technology, Jahangirnagar University, Savar -1342, Dhaka, Bangladesh  
biman@buft.edu.bd   
Abstract:  This paper investigates the inclusion of microservices architecture in the development of 
scalable and reliable airline reservation systems. Most of the traditional reservation  systems are very 
rigid  and centralized which makes them prone to bottlenecks and a single point of failure. As such, 
systems do not meet the requirements of modern airlines which are dynamic. Microservices offer better 
resiliency and scalability because the services do not depe nd on one another and can be deployed 
independently.  
The approach is grounded on the Circuit Breaker Pattern to maintain fault tolerance while consuming 
foreign resources such as flight APIs and payment systems. This avoided the failure propagation to the 
systems by 60% enabling the systems to function under external failures. Traffic rerouting also bolstered 
this with a guarantee of above 99.95% uptime in systems where high availability was demanded.  
To address this, load balancing was used, particu larly the Round -Robin method which managed to 
enhance performance by 35% through the equal distribution of user requests among the service 
instances. Health checks, as well as monitoring in real -time, helped as well with failure management as 
they helped t o contain failures before the users of the system were affected.  
The results suggest that the use of microservices led to a 40% increase in system scalability, a 50% 
decrease in downtime and a support for 30% more concurrent users than the use of monolithi c 
architectures. These findings affirm the capability of microservices in the development of robust and 
flexible airline ticket booking systems that are responsive to change and recover from external system 
unavailability.  
Keywords:  Microservices Architect ure, Scalability and Resilience, Load Balancing , Online Travel 
Agent, Microservices, Cloud C mputing, Service Discovery Techniques.  
1. Introduction  
1.1. Background  
The evolution of information and communications technologies in the tourism sector has changed the wa y 
customers interact with travel services. For the sake of real -time availability and tailored customer journey, 
coupled with millions of travel platform users, it is obvious that simply implementing a traditional 
monolithic structure is not sufficient to cater for the increasing complexity and scale which modern travel 
systems mandate. As for monolithic systems, all flight reservations, hotel bookings, payment services and 
customer interactions are inter -linked within one system posing scalability and main tenance issues. Such 
architectures are ill -suited for adaptive traffic patterns, damage containment, or incorporation of new 
features in a s taggered approach [1 1]. 
 
Microservices  architecture has become a standard solution in developing a robust and extensible travel 
booking system . Microservices decompose a monolithic application into smaller self -contained 
applications that communicate with each other using lightweight protocols , thus improving agility, 
scalability and fault tolerance [5]. Each service handles one aspect of the system such as flight searching, "
"IPPON: Common Sense Guided Informative Path
Planning for Object Goal Navigation
Kaixian Qu1, Jie Tan2, Tingnan Zhang2, Fei Xia2, Cesar Cadena1, Marco Hutter1
Object of Interest
hat“I think I forgot my hat on the
sofa. I left it there yesterday
when I was taking a nap.”sofa: certain
coat rack: near
chair: average
fridge: far ...
 Semantic
 SegmentationCommon Objects
sofa, coat rack,
chair , fridge, ...Probability Mapping LLM Reasoning
Sensory InputInformative Path PlanningObject Proximity
sofa certain
coat rack near
chair average
fridge far
...
Fig. 1: The robot receives a task of finding “hat”, informed by the context: “I think I forgot my hat on the sofa. I left it
there yesterday when I was taking a nap.” With the common sense reasoning from an LLM, the robot focuses its search
around the sofa area rather than heading towards the kitchen. We visualize the robot’s trajectory with a red arrow indicating
its direction of movement.
Abstract — Navigating efficiently to an object in an unex-
plored environment is a critical skill for general-purpose intel-
ligent robots. Recent approaches to this object goal navigation
problem have embraced a modular strategy, integrating classi-
cal exploration algorithms—notably frontier exploration—with
a learned semantic mapping/exploration module. This paper
introduces a novel informative path planning and 3D object
probability mapping approach. The mapping module computes
the probability of the object of interest through semantic seg-
mentation and a Bayes filter. Additionally, it stores probabilities
for common objects, which semantically guides the exploration
based on common sense priors from a large language model.
The planner terminates when the current viewpoint captures
enough voxels identified with high confidence as the object of
interest. Although our planner follows a zero-shot approach,
it achieves state-of-the-art performance as measured by the
Success weighted by Path Length (SPL) and Soft SPL in the
Habitat ObjectNav Challenge 2023, outperforming other works
by more than 20%. Furthermore, we validate its effectiveness
on real robots. Project webpage: https://ippon-paper.github.io/
I. I NTRODUCTION
Imagine asking an intelligent robot, “Can you fetch me
an apple?” The robot’s response involves initially navigating
to find the apple, picking it up, returning to your location,
and finally delivering it. In a more practical scenario, the
robot lacks prior knowledge of the apple’s location and
must actively explore a cluttered, unknown environment to
locate it. This problem is usually referred to as Object Goal
Navigation [1], where, given a description of the object of
interest (OOI), the agent must decide where to explore and
when to terminate within an unknown map.
The success of a navigation task hinges on three pivotal
components: semantic mapping that identifies the placement
This work was supported by an ETH RobotX research grant funded
through the ETH Zurich Foundation, by the Swiss National Science Foun-
dation through the National Centre of Competence in Digital Fabrication
(NCCR dfab), and by Huawei Tech R&D (UK) through a research funding
agreement. Moreover, this work has been conducted as part of ANYmal
Research, a community to advance legged robotics.
1Robotic Systems Lab, ETH Zurich, Switzerland. e-mail: {kaixqu,
cesarc, mahutter }@ethz.ch. Corresponding author: Kaixian Qu.
2Google DeepMind, USA. e-mail: {jietan, tingnan, xiafei }@google.com.of objects, planning that determines exploration goals with
the shortest traversable paths, and, equally important, seman-
tic guidance that predicts the probable locations of the object
of interest with common-sense reasoning.
The recent development of photo-realistic simulators and
reliable object detection/semantic segmentation has matured
the semantic mapping technique. This is evident in the
substantial growth of the success rate, increasing from 14%
in Habitat ObjectNav 2020 to 59% in Habitat ObjectNav
2023. This improvement indicates a significant reduction
in the likelihood of either failing to detect the OOI or
mistakenly identifying other objects as the OOI.
Recent work in object goal navigation has recognized
the importance of exploration planners, with frontier-based
exploration (FBE) [2] being the predominant choice. How-
ever, the field of autonomous exploration has shifted its
focus towards integrating the next best view [3] with the
sampling-based planning (SBP) [4], [5]. This integration [6]
allows robots to explore 3D spaces, whereas FBE is typically
confined to 2D maps.
Semantic guidance is key in helping robots to find objects
efficiently. For example, if a robot spots a “chest of drawers”
on a map, it is more likely to discover a “bed” in nearby
unexplored areas. Conversely, the presence of a counter
suggests that a bed is likely to be somewhere far away.
The recent development of large language models (LLMs)
has further advanced this area by enabling robots to handle
open-vocabulary objects and understand complex contextual
instructions, such as “I think I forgot my hat on the sofa.”
This paper introduces IPPON, which adapts an online
informative path planning (IPP) framework [7] to address ob-
ject goal navigation problems. Unlike volumetric exploration,
where the gain of each viewpoint is measured by the number
of unknown voxels observed, we define gain as the aggre-
gated probability of voxels that are likely part of the OOI.
To enable such gain computation, we introduce a 3D object
probability mapping algorithm comprising two components:
one estimating probabilities for common objects and anotherarXiv:2410.19697v1  [cs.RO]  25 Oct 2024"
"Less is More: Extreme Gradient Boost Rank-1
Adaption for Efficient Finetuning of LLMs
Yifei Zhang1,5, Hao Zhu2, Aiwei Liu3, Han Yu1, Piotr Koniusz2,4and Irwin King5
1Nanyang Technological University,2Data61 & CSIRO,3Tsinghua University
4Australian National University5The Chinese University of Hong Kong
yifei.zhang@ntu.edu.sg
Abstract
Fine-tuning Large Language Models (LLMs) has become a crucial technique
for adapting pre-trained models to downstream tasks. However, the enormous
size of LLMs poses significant challenges in terms of computational complexity
and resource requirements. Low-Rank Adaptation (LoRA) has emerged as a
promising solution. However, there exists a gap between the practical performance
of low-rank adaptations and its theoretical optimum. In this work, we propose
eXtreme Gradient Boosting LoRA (XGBLoRA ), a novel framework that bridges
this gap by leveraging the power of ensemble learning. Inspired by gradient
boosting, XGBLoRA iteratively learns and merges a sequence of LoRA adaptations
to refine model predictions. It achieves better performance than the standard LoRA,
while enjoying the computational efficiency of rank-1 adaptations. We provide
theoretical analysis to show the convergence and optimality of our approach, and
conduct extensive experiments on a range of natural language processing tasks.
The results demonstrate that XGBLoRA consistently outperforms standard LoRA
and achieves performance comparable to full fine-tuning with significantly fewer
trainable parameters. This work advances parameter-efficient fine-tuning for LLMs,
and offers a promising solution for adapting LLMs to downstream tasks while
optimizing performance and efficiency.
1 Introduction
101
100101102103
#Params(M)85.085.586.086.587.087.5GLUE Avg.Fully FT
BitFitHAdapter
PAdapterDeltaLoRAMELoRA
LoRAAdaLoRAFLoRADoRAXGBLoRA
 (Ours)
ReLoRA     High 
Performace
  Param.
 Efficient
LoRA XGBLoRA0.600.610.620.630.640.65Speed/batch (Sec.)
Figure 1: Efficiency vs. effectiveness on
the GLUE dataset. Our XGBLoRA enjoys
high average and uses fewer parameters
than competitors. Mini-figure: speed in
seconds per batch.Large language models (LLMs) have achieved re-
markable success in various natural language process-
ing tasks, enabling breakthroughs in language under-
standing, generation, and reasoning [ 7,35,4]. Simi-
lar to Self-Supervised Learning methods in other do-
mains [ 58,42,57,31,56,59], LLMs are typically pre-
trained on vast amounts of unlabeled text data, and then
fine-tuned on specific downstream tasks to adapt their
knowledge to the target domain [ 48,38,49]. However,
the enormous size of LLMs, often reaching billions of
parameters, poses significant challenges in terms of com-
putational complexity and resource requirements during
fine-tuning [24, 4].
To address these challenges, a promising direction called
parameter-efficient fine-tuning (PEFT) [ 16,50,18]
adapts LLMs to downstream tasks while minimizing
the number of trainable parameters, thereby reducing
Preprint. Under review.arXiv:2410.19694v1  [cs.CL]  25 Oct 2024"
"MILES: Making Imitation Learning Easy
with Self-Supervision
Georgios Papagiannis and Edward Johns
The Robot Learning Lab
Imperial College London, UK
g.papagiannis21@imperial.ac.uk
Abstract: Data collection in imitation learning often requires significant, labori-
ous human supervision, such as numerous demonstrations, and/or frequent envi-
ronment resets for methods that incorporate reinforcement learning. In this work,
we propose an alternative approach, MILES: a fully autonomous, self-supervised
data collection paradigm, and we show that this enables efficient policy learn-
ing from just a single demonstration and a single environment reset. MILES au-
tonomously learns a policy for returning to and then following the single demon-
stration, whilst being self-guided during data collection, eliminating the need
for additional human interventions. We evaluated MILES across several real-
world tasks, including tasks that require precise contact-rich manipulation such
as locking a lock with a key. We found that, under the constraints of a single
demonstration and no repeated environment resetting, MILES significantly out-
performs state-of-the-art alternatives like imitation learning methods that leverage
reinforcement learning. Videos of our experiments and code can be found on our
webpage: www.robot-learning.uk/miles.
Figure 1 :(a) Behavioural cloning from a single demonstration fails to generalize to states outside the demonstration, due
to covariate shift. (b) Providing multiple demonstrations addresses this, but requires significant human effort. (c) While
incorporating reinforcement learning addresses the issue of covariate shift and the need for multiple demonstrations, it re-
quires frequent environment resetting and is highly inefficient due to random exploration. (d) In MILES, we propose a new
self-supervised paradigm that overcomes these issues and can learn a range of complex tasks from a single demonstration
and no additional human effort, by collecting augmentation trajectories that guide the robot back to the demonstration.
1 Introduction
Imitation learning is frequently described as a convenient way to teach robots new skills. But is
this true in practice? Behavioral cloning (BC) methods leverage supervised learning to train robust
policies, but doing so typically requires hundreds or thousands of demonstrations per task [1, 2] to
collect a sufficiently diverse training dataset. Imitation learning methods that leverage Reinforce-
ment learning (RL) offer a solution to this as policies can be learned autonomously through random
8th Conference on Robot Learning (CoRL 2024), Munich, Germany.arXiv:2410.19693v1  [cs.RO]  25 Oct 2024"
"AGENT-CQ: Automatic Generation and Evaluation of Clarifying Questions
for Conversational Search with LLMs
Clemencia Siro‡ Yifei Yuan§ Mohammad Aliannejadi‡ Maarten de Rijke‡
‡University of Amsterdam
§University of Copenhagen
‡{c.n.siro, m.aliannejadi, m.derijke}@uva.nl
§yiya@di.ku.dk
Abstract
Generating diverse and effective clarifying
questions is crucial for improving query
understanding and retrieval performance
in open-domain conversational search ( CS)
systems. We propose AGENT-CQ (Automatic
GEN eration, and evalua Tion of Clarifying
Questions), an end-to-end LLM-based frame-
work addressing the challenges of scalability
and adaptability faced by existing methods that
rely on manual curation or template-based ap-
proaches. AGENT-CQ consists of two stages:
a generation stage employing LLM prompting
strategies to generate clarifying questions, and
an evaluation stage (CrowdLLM) that simu-
lates human crowdsourcing judgments using
multiple LLM instances to assess generated
questions and answers based on comprehensive
quality metrics. Extensive experiments on
the ClariQ dataset demonstrate CrowdLLM’s
effectiveness in evaluating question and answer
quality. Human evaluation and CrowdLLM
show that AGENT-CQ – generation stage,
consistently outperforms baselines in various
aspects of question and answer quality. In
retrieval-based evaluation, LLM-generated
questions significantly enhance retrieval effec-
tiveness for both BM25 and cross-encoder mod-
els compared to human-generated questions.
We will make publicly available the data, labels
and prompts used by AGENT-CQ framework.
1 Introduction
Conversational search ( CS) systems have gained
significant attention in recent years, offering users
a more natural and interactive way to find infor-
mation than single-shot search interactions (Alian-
nejadi et al., 2019; Radlinski and Craswell, 2017;
Zamani et al., 2023). To resolve the ambiguity
inherent in user queries, these systems may ask
users clarifying questions (Zamani et al., 2020b).
Generating diverse and effective clarifying ques-
tions is crucial for improving query understandingGenerationHuman data
(Query (q),
Facet (f))Generate clarifying questions (cq)Clarifying
question pool
Filtering
Ranked cqs Generate answers (a)Synthetic data
(cq,a)q cq
(cq, f, q) (cq, a)Phase 1 - Clarifying question generation
Phase 2 - Question filteringPhase 3 - Answer generation
EvaluationCrowdLLM(cq,a)Question evaluation
Answer evaluation
Figure 1: The AGENT-CQ framework for generating
clarifying questions and simulating answers (top) and
evaluating generated questions and answers (bottom).
and retrieval performance, which remains a chal-
lenge (Yuan et al., 2024).
Existing methods for generating clarifying ques-
tions in CSsystems rely on manual curation by ex-
perts and template-based approaches (Aliannejadi
et al., 2020; Zamani et al., 2020a): human experts
craft clarifying questions, while using their abil-
ity to intuitively understand complex user intents
and contextual nuances. While this method ensures
high relevance and accuracy, it poses challenges
for scalability in large-scale applications (Deriu
et al., 2021). Moreover, human curators do not
necessarily have deep knowledge about the topic of
a conversation. In contrast, template-based meth-
ods employ pre-defined templates to automate the
generation of clarifying questions, significantly en-
hancing scalability and efficiency. However, these
methods often lack flexibility, leading to generic
or less-diverse questions that could hurt the overall
user interaction experience (Yao et al., 2012).
Recently, large language models ( LLM s) havearXiv:2410.19692v1  [cs.CL]  25 Oct 2024"
"Deep learning-based identification of patients at
increased risk of cancer using routine laboratory
markers
Vivek Singh1,*, Shikha Chaganti1, Matthias Siebert2, Soumya Rajesh1, Andrei Puiu3,4, Raj
Gopalan5, Jamie Gramz6, Dorin Comaniciu1, and Ali Kamen1
1Siemens Healthineers, Digital Technology and Innovation, Princeton, 08540, USA
2Siemens Healthineers, Digital Technology and Innovation, Erlangen, 91052, Germany
3Siemens SRL, Advanta, Brasov, 500007, Romania
4Transylvania University of Brasov, Automation and Information Technology, Brasov, 500174, Romania
5Siemens Healthineers, Laboratory Diagnostics, Tarrytown, NY 10591, USA.
6Siemens Healthineers, Digital and Automation, Malvern, PA 19355, USA.
*vivek-singh@siemens-healthineers.com
ABSTRACT
Early screening for cancer has proven to improve the survival rate and spare patients from intensive and costly treatments
due to late diagnosis. Cancer screening in the healthy population involves an initial risk stratification step to determine the
screening method and frequency, primarily to optimize resource allocation by targeting screening towards individuals who
draw most benefit. For most screening programs, age and clinical risk factors such as family history are part of the initial risk
stratification algorithm. In this paper, we focus on developing a blood marker-based risk stratification approach, which could be
used to identify patients with elevated cancer risk to be encouraged for taking a diagnostic test or participate in a screening
program. We demonstrate that the combination of simple, widely available blood tests, such as complete blood count and
complete metabolic panel, could potentially be used to identify patients at risk for colorectal, liver, and lung cancers with areas
under the ROC curve of 0.76, 0.85, 0.78, respectively. Furthermore, we hypothesize that such an approach could not only be
used as pre-screening risk assessment for individuals but also as population health management tool, for example to better
interrogate the cancer risk in certain sub-populations.
Introduction
This paper focuses on the use of multiple biomarkers for the assessment of patients, or identification of otherwise healthy
individuals, who are at increased risk of cancer. With the high mortality rate associated with cancer patients, significant research
has been conducted to help identify patients at higher risk, starting with identifying medical conditions that increase the risk of
cancer, such as diabetes, or genetic predispositions that promote its development1. Furthermore, various screening procedures
have been developed to help facilitate early diagnosis such as the Faecal Immunochemical Test (FIT) and colonoscopy for
colorectal cancer (CRC)2, mammography for breast cancer3, and low-dose computed tomography (LDCT) for lung cancer4.
However, cancer screening rates and their uptake remains lower than desired, e.g., in the US5. While there are several factors
contributing to this low uptake, one of the key factors is the lack of awareness within the general population. This is even
more important to address for people who may be at increased risk and would benefit from early and/or regular screening. In
other words, there is still a need for convenient tests for early detection of rapidly progressing diseases such as cancer so that
intervention can start as early as possible6.
Several cancer risk prediction/assessment tools based on demographic, socioeconomic or blood based markers have been
developed over the years, and studies have shown that cancer risk assessment algorithms could have an impact in early cancer
diagnosis7. For instance, the Qcancer 10 year risk algorithm8considers the age, ethnicity, deprivation, body mass index,
smoking, alcohol, previous cancer diagnoses, family history of cancer, relevant comorbidities, and medication data for a patient
and predicts the cancer risk for 11 types of cancers. Nartowt et al.9reported high concordance in the prediction of CRC into
low, medium and high groups using an artificial neural network trained on patient data comprising age, sex, and complete blood
count (CBC). ColonFlag10can be used to identify individuals at high risk of CRC using specific blood-based markers and refer
them to screening procedures such as colonoscopy. More recently, a cell-free DNA-based blood test for the early detection of
CRC has been clinically validated in the ECLIPSE study11. Moreover, multi-cancer early detection technologies12such as thearXiv:2410.19646v1  [cs.LG]  25 Oct 2024"
"Impact of Leakage on Data Harmonization in Machine Learning Pipelines in Class
Imbalance Across Sites.
Nicol ´as Nieto1,2,3,*, Simon B. Eickhoff1,2, Christian Jung3,4, Martin Reuter5,6, Kersten Diers5,
Malte Kelm3,4, Artur Lichtenberg7, Federico Raimondo1,2, and Kaustubh R. Patil1,2
1Institute of Neuroscience and Medicine (INM-7: Brain and Behaviour), Research Centre J ¨ulich,
J¨ulich, Germany
2Institute of Systems Neuroscience, Heinrich Heine University D ¨usseldorf, D ¨usseldorf, Germany
3Department of Cardiology, Pulmonology and Vascular Medicine, University Hospital and
Medical Faculty, Heinrich-Heine University, Duesseldorf, Germany
4Cardiovascular Research Institute D ¨usseldorf (CARID), Medical Faculty, Heinrich-Heine
University, Duesseldorf, Germany
5Artificial Intelligence in Medical Imaging, German Center for Neurodegenerative Diseases
(DZNE), Bonn, Germany
6Department of Radiology, Harvard Medical School, Boston, MA, USA
7Department of Cardiac Surgery, University Hospital and Medical Faculty, Heinrich-Heine
University, Duesseldorf, Germany
*Correspondence: n.nieto@fz-juelich.de
Summary
Machine learning (ML) models benefit from large datasets. Collecting data in biomedical do-
mains is costly and challenging, hence, combining datasets has become a common practice.
However, datasets obtained under different conditions could present undesired site-specific vari-
ability. Data harmonization methods aim to remove site-specific variance while retaining biolog-
ically relevant information. This study evaluates the effectiveness of popularly used ComBat-
based methods for harmonizing data in scenarios where the class balance is not equal across
sites. We find that these methods struggle with data leakage issues. To overcome this problem,
we propose a novel approach “PrettYharmonize”, designed to harmonize data by pretending the
target labels. We validate our approach using controlled datasets designed to benchmark the
utility of harmonization. Finally, using real-world MRI and clinical data, we compare leakage-
prone methods with “PrettYharmonize” and show that it achieves comparable performance while
avoiding data leakage, particularly in site-target-dependence scenarios.
Keywords
Data Harmonization, ComBat, Data leakage, Machine learning, Medical Imaging, Magnetic Res-
onance Imaging, medical AI, clinical, ICU.
Introduction
Many research fields have greatly benefited from machine learning (ML) approaches. ML models
can extract important values from large amounts of data. Having vast data benefits the model’s
classification performance and helps capture the underlying patterns, promoting better gener-
alization to new unseen data. This makes combining multiple datasets an appealing approach,
especially in domains where obtaining data in a uniform setting is challenging1. Moreover, small
health or research centers that can not afford to collect a large number in-house data, using data
acquired in different sites is the only possibility for train ML models. However, different datasets
1arXiv:2410.19643v2  [cs.LG]  28 Oct 2024"
"OpenWebVoyager: Building Multimodal Web Agents via Iterative
Real-World Exploration, Feedback and Optimization
Hongliang He1,3*, Wenlin Yao2†, Kaixin Ma2, Wenhao Yu2, Hongming Zhang2,
Tianqing Fang2,Zhenzhong Lan3,Dong Yu2
1Zhejiang University,2Tencent AI Lab (Seattle),3Westlake University
Abstract
The rapid development of large language and
multimodal models has sparked significant in-
terest in using proprietary models, such as GPT-
4o, to develop autonomous agents capable of
handling real-world scenarios like web naviga-
tion. Although recent open-source efforts have
tried to equip agents with the ability to explore
environments and continuously improve over
time, they are building text-only agents in syn-
thetic environments where the reward signals
are clearly defined. Such agents struggle to
generalize to realistic settings that require mul-
timodal perception abilities and lack ground-
truth signals. In this paper, we introduce an
open-source framework designed to facilitate
the development of multimodal web agent that
can autonomously conduct real-world explo-
ration and improve itself. We first train the
base model with imitation learning to gain the
basic abilities. We then let the agent explore
the open web and collect feedback on its trajec-
tories. After that, it further improves its policy
by learning from well-performing trajectories
judged by another general-purpose model. This
exploration-feedback-optimization cycle can
continue for several iterations. Experimental
results show that our web agent successfully im-
proves itself after each iteration, demonstrating
strong performance across multiple test sets.1
1 Introduction
Developing autonomous agents that can complete
complex tasks such as web navigation has been a
significant challenge for the AI community (Zhou
et al., 2023; Gur et al., 2023; Deng et al., 2024; Koh
et al., 2024). Recent advancements of large lan-
guage and multimodal models such as Claude (An-
thropic, 2024) and GPT-4o (OpenAI, 2024) have
*Work done during the internship at Tencent AI Lab.
†Work done while at Tencent AI Lab.
1Code and data will be released at https:
//github.com/MinorJerry/OpenWebVoyager . Contact:
hehongliang@westlake.edu.cnmade it possible to build such agents via prompt
engineering (He et al., 2024; Zheng et al., 2024b;
Ma et al., 2023). However, these agents struggle
to improve further due to their reliance on closed-
source models. Another line of work has explored
alternative ways to build agents by starting off with
weaker open-source models and gradually improv-
ing model performance by iteratively exploring the
environment, collecting feedback signals, and up-
dating the policy model (Xi et al., 2024; Putta et al.,
2024; Patel et al., 2024). However, existing studies
have only focused on building text-only agents in
synthetic environments (Song et al., 2024). The
synthetic environments provide the benefit of well-
defined reward signals, allowing the agents to ef-
fectively differentiate the quality of the trajectories
and learn accordingly. However, synthetic environ-
ments fail to capture the complexity of real-world
scenarios, leading to potential generalization is-
sues when applied to real-world tasks. Moreover,
real-world environments often do not have built-
in reward signals, which poses another challenge
in agent’s learning and improvement process (He
et al., 2024). Additionally, real-world webpages
are designed based on human visual preference, ig-
noring the visual inputs can cause significant infor-
mation loss that impacts the agent’s performance.
To address above limitations and explore open-
source models in real-world settings, we propose
OpenWebV oyager, an open-source framework for
building multimodal web agents via iterative real-
world exploration, feedback and optimization. We
show that OpenWebV oyager can learn to perform
real-world web navigation tasks through an initial
imitation learning (IL) phase followed by multiple
exploration-feedback-optimization cycles. To do
so, we start by compiling a diverse set of web task
queries and collecting corresponding agent trajecto-
ries using a state-of-the-art multimodal agent Web-
V oyager (He et al., 2024) based on GPT-4o, which
we refer to as WebV oyager-4o. During the imi-arXiv:2410.19609v1  [cs.CL]  25 Oct 2024"
"CoqPilot , a plugin for LLM-based generation of proofs
Andrei Kozyrev
JetBrains Research
Germany
Constructor University
Bremen, GermanyGleb Solovev
JetBrains Research
Germany
Constructor University
Bremen, GermanyNikita Khramov
JetBrains Research
Germany
Constructor University
Bremen, GermanyAnton Podkopaev
JetBrains Research
the Netherlands
Constructor University
Bremen, Germany
ABSTRACT
We present CoqPilot , a VS Code extension designed to help auto-
mate writing of Coq proofs. The plugin collects the parts of proofs
marked with the admit tactic in a Coq file, i.e.,proof holes, and
combines LLMs along with non-machine-learning methods to gen-
erate proof candidates for the holes. Then, CoqPilot checks if each
proof candidate solves the given subgoal and, if successful, replaces
the hole with it. The focus of CoqPilot is twofold. Firstly, we want
to allow users to seamlessly combine multiple Coq generation ap-
proaches and provide a zero-setup experience for our tool. Secondly,
we want to deliver a platform for LLM-based experiments on Coq
proof generation. We developed a benchmarking system for Coq
generation methods, available in the plugin, and conducted an ex-
periment using it, showcasing the framework’s possibilities. Demo
ofCoqPilot is available at: https://youtu.be/oB1Lx-So9Lo. Code
at: https://github.com/JetBrains-Research/coqpilot
KEYWORDS
LLM, Coq, code generation
ACM Reference Format:
Andrei Kozyrev , Gleb Solovev , Nikita Khramov , and Anton Podkopaev
. 2024. CoqPilot , a plugin for LLM-based generation of proofs. In .ACM,
New York, NY, USA, 4 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn
1 INTRODUCTION
Testing has always been essential for making reliable software. For
some specific domains, such as aerospace engineering, banking
infrastructure, or medical devices, bugs in critical systems may lead
to catastrophic consequences [ 6,12]. Formal software verification
ensures that software operates correctly and safely by proving its
correctness against the specification [ 14]. Under an assumption
of a well-constructed specification, formal verification provides
stronger guarantees than traditional testing methods, such as unit
or integration testing, due to its exhaustive nature. To date, there
exist a number of interactive theorem provers (ITP), such as Coq [ 2],
Isabelle [ 13], or Lean [ 5]. They are designed to assist users with
the construction of formal specifications and verification of for-
mal proofs. For example, Coqhelps in development by providing a
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
Conference’17, July 2017, Washington, DC, USA
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM
https://doi.org/10.1145/nnnnnnn.nnnnnnnrobust framework for defining mathematical assertions and ensur-
ing the logical consistency of complex formal proofs. The formal
verification approach has proved to be fruitful: for instance, Com-
pCert [ 11], a C compiler written in Coq, was the only C compiler
in which an extensive study found no bugs [20].
Coq is an interactive proof system, where proofs are constructed
step-by-step using so-called tactics . When applied, tactics change
the state of the current proof. In particular, a tactic may apply an
already proven lemma, destruct the assumption to perform case
analysis, apply induction reasoning, and much more. At any point of
the proof, the proof state shown to the user will contain information
about the current target statement and the assumptions under
which it has to be proven. When the statement is empty, the proof
is complete. If the proof contains an error or is not constructed
correctly, Coq’s system will tell that the proof is invalid and provide
comprehensive information on the origin of the problem.
Writing formal proofs is an exceptionally time-consuming task
and requires considerable experience from the programmer [ 15].
Various approaches for Coq generation are already present, both
machine-learning-based and not. CoqHammer [ 4] translates the
Coq’s logic into untyped first-order logic and searches for the proof.
The K-NN approach, implemented as a back-end in Tactician [ 3],
predicts tactics based on what has been used in similar cases. Other
approaches are based on generative models [ 8,16,17,19]. Recently,
Large Language Models (LLMs) have gained strong code-generation
capabilities [ 10]. Combined with tools for automatic code verifica-
tion, we may produce high-quality, reliable code seamlessly.
Some developed models and tools for Coq generation may re-
quire significant setup and/or lack integration into the platform for
end users [ 8,17,19]. One other space of improvement for existing
non-deterministic proof search processes is to use the information
provided by the Coq’s system. Even for a human, writing Coq code
in a notepad instead of a proper Coq IDE would be harder than in a
typical programming language. Interactive stepping through each
tactic invocation and updated goal states provide the necessary
information while writing proofs. Fortunately, such information
can be gathered automatically and used for proof generation.
We propose CoqPilot , a VSCode plugin designed to deliver a
convenient generation of Coq code using LLMs and other methods.
We studied possible external enhancements to generating Coq code
with general-purpose models. The automatic checking of multiple
generated proof candidates was developed to pick and present only
the valid one to the user. We implemented premise selection for
better LLM prompting and created an LLM-guided mechanism that
attempts fixing failing proofs with the help of the Coq’s error mes-
sages. To evaluate the performance of the described solutions, we
implemented a benchmarking framework for our extension. ThearXiv:2410.19605v1  [cs.SE]  25 Oct 2024"
"Take Caution in Using LLMs as Human Surrogates:
Scylla Ex Machina∗
Yuan Gao
Questrom School of Business
Information Systems Department
Boston University
Boston, MA 02215
yuangg@bu.eduDokyun Lee
Questrom School of Business
Information Systems Department and
Computing & Data Sciences
Boston University
Boston, MA 02215
dokyun@bu.edu
Gordon Burtch
Questrom School of Business
Information Systems Department
Boston University
Boston, MA 02215
gburtch@bu.eduSina Fazelpour
Department of Philosophy and
Khoury College of Computer Sciences
Northeastern University
Boston, MA 02115
s.fazel-pour@northeastern.edu
This Version: Oct 24, 2024†
Abstract
Recent studies suggest large language models (LLMs) can exhibit human-like reasoning,
aligning with human behavior in economic experiments, surveys, and political discourse.
This has led many to propose that LLMs can be used as surrogates for humans in social
science research. However, LLMs differ fundamentally from humans, relying on prob-
abilistic patterns, absent the embodied experiences or survival objectives that shape hu-
man cognition. W e assess the reasoning depth of LLMs using the 11-20 money request
game. Almost all advanced approaches fail to replicate human behavior distributions
across many models, except in one case involving fine-tuning using a substantial amount
of human behavior data. Causes of failure are diverse, relating to input language, roles,
and safeguarding. These results caution against using LLMs to study human behaviors
or as human surrogates.
‘She has twelve misshapen feet, and six necks of the most prodigious length;
and at the end of each neck she has a frightful head with three rows of teeth in each’
— Homer, Odyssey (Describing Scylla)
Introduction
Recent studies report that Large Language Models (LLMs) can exhibit human-like cognitive abilities.
These studies demonstrate that LLMs show behaviors that align closely with those of human subjects
†Previous V ersion: Aug 28 and Oct 12, 2024
∗W e thank seminar participants at the 2nd AI at Wharton W orkshop (Sep 2024) and the 8th annual Psychology of T echnology Institute
conference (Oct 2024). All errors are the author’s own.arXiv:2410.19599v1  [econ.GN]  25 Oct 2024"
"Connecting Joint-Embedding Predictive Architecture
with Contrastive Self-supervised Learning
Shentong Mo1∗, Shengbang Tong2
1CMU,2NYU
Abstract
In recent advancements in unsupervised visual representation learning, the Joint-
Embedding Predictive Architecture (JEPA) has emerged as a significant method for
extracting visual features from unlabeled imagery through an innovative masking
strategy. Despite its success, two primary limitations have been identified: the
inefficacy of Exponential Moving Average (EMA) from I-JEPA in preventing entire
collapse and the inadequacy of I-JEPA prediction in accurately learning the mean
of patch representations. Addressing these challenges, this study introduces a novel
framework, namely C-JEPA (Contrastive-JEPA), which integrates the Image-based
Joint-Embedding Predictive Architecture with the Variance-Invariance-Covariance
Regularization (VICReg) strategy. This integration is designed to effectively learn
the variance/covariance for preventing entire collapse and ensuring invariance
in the mean of augmented views, thereby overcoming the identified limitations.
Through empirical and theoretical evaluations, our work demonstrates that C-JEPA
significantly enhances the stability and quality of visual representation learning.
When pre-trained on the ImageNet-1K dataset, C-JEPA exhibits rapid and improved
convergence in both linear probing and fine-tuning performance metrics.
1 Introduction
Figure 1: Our C-JEPA achieves faster and
better convergence than I-JEPA.Unsupervised learning of visual representations has
recently seen remarkable progress, primarily due
to the development of innovative architectures and
strategies that exploit unlabeled imagery. Among
these advancements, the Joint-Embedding Predictive
Architecture (JEPA) [ 1,2,3] has distinguished it-
self as a powerful approach. I-JEPA [ 2] leverages a
masking strategy to extract visual features, facilitat-
ing significant strides in understanding and utilizing
unlabeled visual data.
However, despite its successes, certain limitations
within the JEPA framework have become apparent,
particularly concerning its components I-JEPA Expo-
nential Moving Average (EMA) and I-JEPA predic-
tion capabilities. Specifically, I-JEPA EMA has been
found to be inadequate in preventing the issue of entire collapse [ 4,5], while the I-JEPA prediction
mechanism struggles to accurately learn the mean of patch representations. These challenges not
only hinder the performance of JEPA but also limit its applicability in broader contexts.
∗Corresponding author: shentongmo@gmail.com .
38th Conference on Neural Information Processing Systems (NeurIPS 2024).arXiv:2410.19560v1  [cs.CV]  25 Oct 2024"
"On Occlusions in Video Action Detection: Benchmark
Datasets And Training Recipes
Rajat Modi1∗, Vibhav Vineet2, Yogesh Singh Rawat1
CRCV , University of Central Florida1, and Microsoft Research2
Abstract
This paper explores the impact of occlusions in video action detection. We fa-
cilitate this study by introducing five new benchmark datasets namely O-UCF
and O-JHMDB consisting of synthetically controlled static/dynamic occlusions,
OVIS-UCF and OVIS-JHMDB consisting of occlusions with realistic motions and
Real-OUCF for occlusions in realistic-world scenarios. We formally confirm an
intuitive expectation: existing models suffer a lot as occlusion severity is increased
and exhibit different behaviours when occluders are static vs when they are moving.
We discover several intriguing phenomenon emerging in neural nets: 1) transform-
ers can naturally outperform CNN models which might have even used occlusion as
a form of data augmentation during training 2) incorporating symbolic-components
like capsules to such backbones allows them to bind to occluders never even seen
during training and 3) Islands of agreement can emerge inrealistic images/videos
without instance-level supervision, distillation or contrastive-based objectives2(eg.
video-textual training). Such emergent properties allow us to derive simple yet
effective training recipes which lead to robust occlusion models inductively satis-
fying the first two stages of the binding mechanism (grouping/segregation). Mod-
els leveraging these recipes outperform existing video action-detectors under oc-
clusion by 32.3% on O-UCF, 32.7% on O-JHMDB & 2.6% on Real-OUCF in
terms of the vMAP metric. The code for this work has been released at https:
//github.com/rajatmodi62/OccludedActionBenchmark .
1 Introduction
Deep learning[ 41] has led to significant advances in object detection/segmentation for both image[ 16,
19] and video domain[ 84]. Such deep neural networks are in turn widely used in self-driving cars and
safety critical scenarios. A key concern for such applications is whether they are able to perform well
when encountering realistic occlusions: e.g. are they able to reliably localize a pedestrian even when
an occluder (say a dog) comes in front of him. However, one major limitations being existing dataset
test split doesn’t contain such occlusions. This raises a concern, whether these models will be robust
to real-world occlusions or not.
One would suspect that the inherent inductive biases of these architectures would be enough to induce
natural occlusion robustness. To verify the hypothesis, we run two preliminary setups: (Fig. 1)
(A) Firstly, superimposing a single occluder (eg bus) over the actor, and, (B) Then, we analyze the
performance if the occlusion is shifted to background (i.e. no occlusion over actor at all). We observe
a relative drop of 20-50% across multiple existing state-of-the-art approaches [ 46,13]. This verifies
∗Corresponding Author, email: rajatmodi@ucf.edu
2Grouping pixels is a perceptual phenomenon of the visual-cortex[ 59] . Assigning a group to a class is a
property of language. If a group is identified by multiple-names, then a neural-net can learn this one-many
mapping via contrastive objectives[ 75]. But grouping should also emerge without language since organisms can
continue to perceive objects even without ears/tongue.
37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks.arXiv:2410.19553v1  [cs.CV]  25 Oct 2024"
"R E S E A R C H A R T I C L E
DeMuVGN: Effective Software Defect Prediction Model by
Learning Multi-view Software Dependency via Graph Neural
Networks
Yu Qiao1Lina Gong1Yu Zhao1Yongwei Wang2Mingqiang Wei1
1College of Computer Science and Technology,
Nanjing University of Aeronautics and
Astronautics, Nanjing, China
2Shanghai Institute for Advanced Study and College
of Computer Science, Zhejiang University,
Shanghai, China
Correspondence
Corresponding author Lina Gong, College of
Computer Science and Technology, Nanjing
University of Aeronautics and Astronautics.
Email: gonglina@nuaa.edu.cn
Present address
College of Computer Science and Technology,
Nanjing University of Aeronautics and Astronautics.Abstract
Software defect prediction (SDP) aims to identify high-risk software defect modules in software development,
allowing resources to be allocated efficiently. Previous studies have demonstrated that dependency network
metrics can improve defect prediction performance. However, cutting-edge methods often rely on program
code view to construct the dependency graph while ignoring the developer factors in software development.
Besides, current dependency network metrics are mainly based on handcrafted metrics (i.e., ego and global
network metrics) to represent the program information, which cannot comprehensively and intelligently
cover relevant defect information. To this end, we propose an effective defect prediction model by learning
multi-view software dependency via graph neural networks, dubbed DeMuVGN. Specifically, we first
propose a Multi-view Software Dependency Graph (MSDG) to combine data, call, and developer dependency.
We then enhance the Bidirectional Gated Graph Neural Network (BiGGNN) by Synthetic Minority Over-
sampling Technique (SMOTE) to better learn the class imbalance program representations for identifying the
defective modules more effectively. Finally, through an extensive case study on eight open-source software
projects across 20 versions, we find that: i) models based on the multi-view dependency graph improve
the F1 score by 11.1% ∼12.1% over models based on the single-view dependency graph. ii) DeMuVGN
improves state-of-the-art methods in within-project context by 17.4% ∼45.8% and cross-project context
by 17.9% ∼41.0% in terms of the F1 score. By analyzing the results, we also discover the advantage of
DeMuVGN in software evolution as the performance of later-stage software versions improves more than
early versions in a within-project context, and observe the generalizability of DeMuVGN as it performs well
in a cross-project context. Based on these findings, we recommend future research to consider using the
multi-view dependency graph to build defect prediction models not only in mature projects with historical
data but also in newly developed projects.
K E Y W O R D S
Software Defect Prediction, Code Dependency Graph, Developer Dependency Graph, Bidirectional Gated
Graph Neural Network
1 INTRODUCTION
Since software defects are inevitable in the software development process1,2,3, it is crucial to promptly identify high-risk software
defect modules to efficiently allocate resources (e.g., budget, time, and personnel)4,5,6. Software defect prediction (SDP) can
apply statistics or machine learning algorithms to learn the relationship between software internal attributes (e.g., code metrics,
process metrics), which has helped developers automatically and quickly discover high-risk defective modules7,8,9,10,11,12.
The dependency graph constructed by code dependencies (e.g., data or call dependencies) provides a good representation of
the defects flow among software modules13. The dependency network metrics can improve the performance of SDP models
14,15. However, as we know, in addition to program code view dependencies among software modules, there are developer
Journal 2024;00:1–21 © 2024 1arXiv:2410.19550v1  [cs.SE]  25 Oct 2024"
"PMM-Net: Single-stage Multi-agent Trajectory Prediction with
Patching-based Embedding and Explicit Modal Modulation
Liu Huajian, Dong Wei, Fan Kunpeng, Wang Chao and Gao Yongzhuo∗
Abstract — Analyzing and forecasting trajectories of agents
like pedestrians plays a pivotal role for embodied intelligent ap-
plications. The inherent indeterminacy of human behavior and
complex social interaction among a rich variety of agents make
this task more challenging than common time-series forecasting.
In this letter, we aim to explore a distinct formulation for
multi-agent trajectory prediction framework. Specifically, we
proposed a patching-based temporal feature extraction module
and a graph-based social feature extraction module, enabling
effective feature extraction and cross-scenario generalization.
Moreover, we reassess the role of social interaction and present
a novel method based on explicit modality modulation to
integrate temporal and social features, thereby constructing
an efficient single-stage inference pipeline. Results on public
benchmark datasets demonstrate the superior performance of
our model compared with the state-of-the-art methods. The
code is available at: github.com/TIB-K330/pmm-net.
I. INTRODUCTION
Multi-agent trajectory prediction (MATP) problem aims to
predict the future trajectories for one or multiple interacting
agents conditioned on their past movements. It serves as
a critical component for autonomous navigation tasks in
dynamic environments, providing essential perception infor-
mation for planners during decision-making and obstacle
avoidance [1]. Compared to conventional time-series fore-
casting (TSF) problem, MATP places greater emphasis on
the accuracy of short-term predictions rather than long-term
trends, while also requiring high real-time performance. In
addition, MATP differs from TSF in two key challenges:
handling complex interactions among agents, and addressing
the inherent indeterminacy of human behavior, which leads
to the multi-modal nature of future states.
Existing mainstream approaches have commonly adopted
Transformer-based architectures that are widely used in NLP
domain [2], leveraging their remarkable capability of ex-
tracting semantic correlations among elements in sequences.
However, recent studies [3] have demonstrated that such
architectures are redundant and inefficient for time-series
forecasting. This is because self-attention is permutation-
invariant, requiring positional encoding or other methods
to preserve temporal information. The semantic information
contained in individual trajectory points is too sparse, leading
to a loss of temporal relationships between tokens in deeper
network layers, which ultimately results in performance
degradation of the entire model. Therefore, it is necessary
to explore more effective feature extraction methods with
lower computational complexity.
All authors are with the State Key Laboratory of Robotics and System,
Harbin Institute of Technology, Harbin, China. ( ∗: Corresponding author)
hjliu@stu.hit.edu.cn, gaoyongzhuo@hit.edu.cnFor multi-modal prediction, existing approaches often rely
on generative models or employ goal-conditioned frame-
works. Generative models represent multi-modality using
a latent variable, with typical examples including gener-
ative adversarial networks (GANs), conditional variational
auto-encoders (CV AEs), and more recent frameworks based
on probabilistic diffusion models (DPMs). Goal-conditioned
methods usually adopt a two-stage training framework, pre-
dicting candidate endpoints then guiding the the regression
of the future trajectory. However, generative models are often
inefficient and struggle to meet the high real-time demands
of autonomous robotic tasks. On the other hand, the two-
stage architecture used in goal-conditioned approaches can
lead to unstable training or produce unnatural trajectories.
To address these critical gaps in MATP problem, we
present a novel approach in this letter. Inspired by recent
advances in TSF [4], we propose a patching-based tempo-
ral feature extraction module. By incorporating sub-series-
level patches, this module enhances locality and captures
comprehensive semantic information that is unavailable at
the point level. In addition, we design a graph-based social
feature extraction module that ensures translational and rota-
tional invariance under series-level normalization, improving
the model’s generalization ability across different scenarios.
Furthermore, we construct a novel single-stage framework
through a cross-attention-based modality modulation using
social information, which decouples temporal and social
features, thereby reducing the overall computational com-
plexity. Extensive experiments demonstrate that our method
accurately predicts plausible future trajectories with multi-
modality, achieving state-of-the-art results on Stanford Drone
and ETH/UCY datasets.
The main contributions of this paper can be summarized
as follows: (1) We propose a sliding-window patching-
based input embedding scheme that addresses the limitation
of semantic correlation loss caused by directly embedding
2D points as tokens, enabling effective temporal feature
extraction for agents; (2) We propose an novel social feature
extraction method that utilizes an inverted attention mecha-
nism among agents, achieving low computational complexity
while ensuring translational and rotational invariance of
the scene; (3) We design a distinct single-stage framework
named PMM-Net that employs a cross-attention-based ex-
plicit modality modulation to achieve multi-modal prediction
for MATP, while more effectively supporting subsequent
decision-making and planning tasks.arXiv:2410.19544v1  [cs.RO]  25 Oct 2024"
"BRAIN -LIKE FUNCTIONAL ORGANIZATION WITHIN
LARGE LANGUAGE MODELS
Haiyang Sun∗*1, Lin Zhao*2, Zihao Wu2, Xiaohui Gao1, Yutao Hu1, Mengfei Zuo1, Wei Zhang3,
Junwei Han1, Tianming Liu2, and Xintao Hu††1
1School of Automation, Northwestern Polytechnical University
2School of Computing, University of Georgia
3School of Computer and Cyber Sciences, Augusta University
shy122@mail.nwpu.edu.cn,liniebest@gmail.com,zw63397@uga.edu,xhgait@mail.nwpu.edu.cn,
HuYutao@mail.nwpu.edu.cn,zuomengfei@mail.nwpu.edu.cn,wzhang2@augusta.edu,
jhan@nwpu.edu.cn, tianming.liu@gmail.com,xhu@nwpu.edu.cn
ABSTRACT
The human brain has long inspired the pursuit of artificial intelligence (AI). Re-
cently, neuroimaging studies provide compelling evidence of alignment between
the computational representation of artificial neural networks (ANNs) and the neu-
ral responses of the human brain to external stimuli, suggesting that ANNs may
employ brain-like information processing strategies. While such alignment has
been observed across sensory modalities—visual, auditory, and linguistic—much
of the focus has been on the behaviors of artificial neurons (ANs) at the popu-
lation level, leaving the functional organization of individual ANs that facilitates
such brain-like processes largely unexplored. In this study, we bridge this gap by
directly coupling sub-groups of artificial neurons with functional brain networks
(FBNs), the foundational organizational structure of the human brain. Specifi-
cally, we extract representative patterns from temporal responses of ANs in large
language models (LLMs), and use them as fixed regressors to construct voxel-
wise encoding models to predict brain activity recorded by functional magnetic
resonance imaging (fMRI). This framework effectively links the AN sub-groups
to FBNs, enabling the delineation of brain-like functional organization within
LLMs. Our findings reveal that LLMs (BERT and Llama 1–3) exhibit brain-like
functional architecture, with sub-groups of artificial neurons mirroring the orga-
nizational patterns of well-established FBNs. Notably, the brain-like functional
organization of LLMs evolves with the increased sophistication and capability,
achieving an improved balance between the diversity of computational behaviors
and the consistency of functional specializations. This research represents the
first exploration of brain-like functional organization within LLMs, offering novel
insights to inform the development of artificial general intelligence (AGI) with
human brain principles.
1 I NTRODUCTION
The human brain, with its unparalleled capacities in perception, cognition, reasoning, and creativity,
stands as the pinnacle of biological intelligence and complexity (Sporns et al., 2000; Bassett &
Gazzaniga, 2011). Understanding the mechanisms behind these cognitive abilities has been one of
the most formidable challenges in neuroscience for decades (Brodmann, 1909; Hubel & Wiesel,
1979; Belliveau et al., 1991; Bear et al., 2020). Despite significant advances, the intricate processes
through which the brain organizes and interprets information—transforming raw sensory inputs into
∗Equal contribution.
†Corresponding author.
1arXiv:2410.19542v1  [q-bio.NC]  25 Oct 2024"
"CLOSERMUSICDB: A MODERN MULTIPURPOSE DATASET OF HIGH
QUALITY MUSIC
Aleksandra Piekarzewicz1Tomasz Sroka1,2Aleksander Tym1Mateusz Modrzejewski1,2
1Closer Music
2Institute of Computer Science, Warsaw University of Technology, Poland
{aleksandra.piekarzewicz, tomasz.sroka, aleksander.tym, mateusz.modrzejewski}@closermusic.com
ABSTRACT
In this paper, we introduce CloserMusicDB , a collection
of full length studio quality tracks annotated by a team
of human experts. We describe the selected qualities of
our dataset, along with three example tasks possible to
perform using this dataset: hook detection, contextual
tagging and artist identification. We conduct baseline
experiments and provide initial benchmarks for these
tasks.
1. INTRODUCTION
Music audio datasets are essential for advancing research
in fields such as music information retrieval, machine
learning, and signal processing. However, the quality of
available datasets varies significantly, often due to copy-
right restrictions and limited access to high-fidelity record-
ings [1]. This variability can affect the reproducibility and
potential for generalization of research findings, under-
scoring the need for more comprehensive and high-quality
music datasets.
We introduce CloserMusicDB, a dataset designed to
provide high-quality, copyright-compliant music for re-
search purposes along with selected subsets of metadata
provided by human experts. Alongside the audio, the
dataset provides hook start and end annotations, a subset of
relevant hashtags and artist identifiers for each audio file.
By ensuring consistent quality and accessibility, Closer-
MusicDB aims to support reproducible research and foster
advancements in music-related machine learning tasks.
2. EXAMPLE TASKS
2.1 Hook detection
Ahook in popular music refers to a short, memorable mu-
sical or lyrical phrase designed to catch the listener’s at-
tention. It is often repeated throughout the song and can
© A. Piekarzewicz, T. Sroka, A. Tym and M. Modrzejew-
ski. Licensed under a Creative Commons Attribution 4.0 International Li-
cense (CC BY 4.0). Attribution: A. Piekarzewicz, T. Sroka, A. Tym and
M. Modrzejewski, “ CloserMusicDB: A Modern Multipurpose Dataset of
High Quality Music”, in Extended Abstracts for the Late-Breaking Demo
Session of the 25th Int. Society for Music Information Retrieval Conf.,
San Francisco, United States, 2024.appear in various sections, such as the chorus, intro, or
bridge. The hook is crafted to be immediately recogniz-
able, driving listener engagement and retention. Its pur-
pose is to enhance recall and it often contributes signifi-
cantly to a song’s commercial success.
Hook detection refers to identifying the most prominent
and repeated segments of a song, often those intended to
capture the listener’s attention. Hooks may vary in instru-
mentation and energy level, making them challenging to
detect. Our dataset contains hook annotations manually la-
beled by expert musicians and producers, who identified
these sections in each track, ensuring accurate identifica-
tion of these sections.
2.2 Contextual tagging
In music tagging, the concept of tags involves assigning
descriptive labels to tracks, albums, or artists to categorize
and organize music based on its characteristics. These tags
can include genre, mood, tempo, instrumentation, or spe-
cific qualities such as ""energetic"" or ""melancholic."" Tags
serve to facilitate search, recommendation, and discovery
of music by highlighting its intrinsic qualities [2].
However, while many tags focus on musical or emo-
tional attributes, relatively few describe the contextual or
functional aspects of music—such as occasions, purposes,
or specific scenarios in which the music may be used.
These ""hashtag"" qualities might include tags like ""work-
out,"" ""travel,"" or ""background for video,"" which go beyond
musical properties to signal practical use cases or associa-
tions. Expanding the tagging system to include such con-
textual tags could enhance the utility of music recommen-
dation systems, as it would allow users to find tracks not
only based on how they sound, but also how they fit spe-
cific activities or multimedia content.
2.3 Artist identification
Artist identification focuses on recognizing distinctive au-
dio patterns that are characteristic of a specific artist. Our
dataset contains multiple examples from the same artist,
enabling models to detect consistent features and stylistic
elements unique to each artist. This allows for the iden-
tification of recurring patterns across works, facilitating
the differentiation between artists based on their individ-
ual musical traits.arXiv:2410.19540v1  [cs.SD]  25 Oct 2024"
"JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2024 1
DMT-HI: MOE-based Hyperbolic Interpretable
Deep Manifold Transformation for Unspervised
Dimensionality Reduction
Zelin Zang∗,Member, IEEE, Yuhao Wang∗,Student Member, IEEE, Jinlin Wu, Student Member, IEEE, Hong
Liu, Member, IEEE, Yue Shen, Member, IEEE, Stan.Z Li†,Fellow, IEEE and Zhen Lei†,Fellow, IEEE
Abstract —Dimensionality reduction (DR) plays a crucial role
in various fields, including data engineering and visualization,
by simplifying complex datasets while retaining essential infor-
mation. However, the challenge of balancing DR accuracy and
interpretability remains crucial, particularly for users dealing
with high-dimensional data. Traditional DR methods often face a
trade-off between precision and transparency, where optimizing
for performance can lead to reduced interpretability, and vice
versa. This limitation is especially prominent in real-world
applications such as image, tabular, and text data analysis,
where both accuracy and interpretability are critical. To address
these challenges, this work introduces the MOE-based Hyper-
bolic Interpretable Deep Manifold Transformation (DMT-HI).
The proposed approach combines hyperbolic embeddings, which
effectively capture complex hierarchical structures, with Mixture
of Experts (MOE) models, which dynamically allocate tasks based
on input features. DMT-HI enhances DR accuracy by leveraging
hyperbolic embeddings to represent the hierarchical nature of
data, while also improving interpretability by explicitly linking
input data, embedding outcomes, and key features through
the MOE structure. Extensive experiments demonstrate that
DMT-HI consistently achieves superior performance in both
DR accuracy and model interpretability, making it a robust
solution for complex data analysis. The code is available at
https://github.com/zangzelin/code dmthi.
Index Terms —Dimensionality Reduction, Mixture of Experts
(MOE), Hyperbolic Embedding, Interpretability
I. I NTRODUCTION
Dimensionality reduction [1], [2], [3], [4] simplifies com-
plex datasets while preserving their intrinsic structure [5], [6].
This is crucial for managing high-dimensional data, which
presents challenges in computational complexity, storage, and
visualisation [7], [8]. Reducing data dimensionality allows
for more efficient analysis, pattern recognition, and interpre-
tation [9]. However, balancing high performance [10] and
interpretability [11], [12] remains challenging, as efficient pro-
cessing [13] often conflicts with human interpretability [14],
[15].
Zelin Zang is with Centre for Artificial Intelligence and Robotics (CAIR),
HKISI-CAS and Westlake University. email: zangzelin@westlake.edu.cn
Yuhao Wang and Stan.Z Li are with Westlake University. Hong Liu
is with School of Information and Electrical Engineering, Hangzhou City
University, Hangzhou, 310015 China and Academy of Edge Intelligence
Hangzhou City University, Hangzhou City University, 310015 China. Jinlin
Wu is with CAIR, HKISI-CAS; State Key Laboratory of Multimodal Artificial
Intelligence Systems (MAIS), CASIA. Zhen Lei is with CAIR, HKISI-CAS;
MAIS, CASIA; and School of Artificial Intelligence, University of Chinese
Academy of Sciences (UCAS). Yue Shen is with Ant Group.
Manuscript received Oct 8, 2024
Manifold-based Dimensionality Reduction (DR) Methods 
Deep Learning -based  Dimensionality Reduction  (DR)  Methods 
Proposed DMT -HITabular  Data
orNo Interpretability Small  Data Large  DataParameter-
free Model
Limited  InterpretabilityTraining
CostsEuclidean
DR
Results
Model Para.Parameter
Model
Higher  InterpretabilityDMT -HI
MOE  Model
e2e3e1
Higher  PerformanceWeak PerformanceWeak Performance
Small  Data Large  DataSmall  Data Large  DataTraining
Costs
Training
CostsEuclidean
DR
Results
Hyperbolic
DR
Results
MOE
Para.Tabular  Data
Tabular  DataImage  Data
Black
Box
Fig. 1. Overview of the proposed DMT-HI network. The figure compares
three dimensionality reduction methods, manifold-based, deep learning-based,
and the proposed DMT-HI. DMT-HI leverages a Mixture of Experts (MOE)
strategy to efficiently process both image [16] and tabular [17] data, offering
better performance, lower training costs, and improved interpretability across
different data sizes.
Dimensionality reduction methods fall into two categories,
manifold-based parameter-free approaches [18], [19], [20]
and deep learning-based methods [21]. Manifold-based meth-
ods like t-SNE [22] and UMAP [23] are known for their
speed (on small dataset) and adaptability [7], projecting high-
dimensional data into low-dimensional spaces through nonlin-
ear mappings, revealing underlying structures. Deep learningarXiv:2410.19504v1  [cs.LG]  25 Oct 2024"
"Peter Parker or Spiderman?
Disambiguating Multiple Class Labels
Nuthan Mummani1, Simran Ketha1,2, Venkatakrishnan Ramaswamy1,2∗
1Department of Computer Science & Information Systems,
Birla Institute of Technology & Science Pilani, Hyderabad 500078, India.
2Anuradha & Prashanth Palakurthi Centre for Artificial Intelligence Research,
Birla Institute of Technology & Science Pilani, Hyderabad 500078, India.
{h20221030057, p20200021, venkat}@hyderabad.bits-pilani.ac.in
Abstract
In the supervised classification setting, during inference, deep networks typically
make multiple predictions. For a pair of such predictions (that are in the top- k
predictions), two distinct possibilities might occur. On the one hand, each of the
two predictions might be primarily driven by two distinct sets of entities in the
input. On the other hand, it is possible that there is a single entity or set of entities
that is driving the prediction for both the classes in question. This latter case, in
effect, corresponds to the network making two separate guesses about the identity
of a single entity type. Clearly, both the guesses cannot be true, i.e. both the
labels cannot be present in the input. Current techniques in interpretability research
do not readily disambiguate these two cases, since they typically consider input
attributions for one class label at a time. Here, we present a framework and method
to do so, leveraging modern segmentation and input attribution techniques. Notably,
our framework also provides a simple counterfactual “proof” of each case, which
can be verified for the input on the model (i.e. without running the method again).
We demonstrate that the method performs well for a number of samples from the
ImageNet validation set and on multiple models.
1 Introduction
Supervised deep learning models performing classification are being widely deployed in many
settings. An important and active direction of research is on interpretability of the predictions of
these models. In the multiclass classification setting, typically, each training datapoint comes with
one or few labels Deng et al .(2009), Lin et al .(2014); however models usually output softmax
prediction ""probabilities"" for every class label present in the dataset. Generally, either the top one or
topksoftmax values are considered as predictions for classification. Since contemporary datasets
have large numbers of labels, many of the labels in the top kpredictions are likely those that aren’t
present in the input in question. Indeed, for a given pair of such predicted labels, these prediction
probabilities have two distinct interpretations. The first interpretation is that the probabilities represent
the possibility of the presence of distinct entities that correspond to each of the class labels. The
second interpretation is that the pair of probabilities represent two distinct predictions about a single
type of entity present in the image. Both these interpretations could simultaneously be true for
different pairs of predicted class labels for a single input that is run through a model. The second
interpretation being true for a given pair of labels might detract from our confidence that both the
labels are indeed correct predictions; this would indicate the need to verify these predictions via
other means – e.g. using a different more capable model or a human. Most contemporary models do
∗Code available in https://github.com/mummani-nuthan/Disambiguating-Multiple-Class-Labels
38th Conference on Neural Information Processing Systems (NeurIPS 2024). ATTRIB Workshop.arXiv:2410.19479v1  [cs.CV]  25 Oct 2024"
